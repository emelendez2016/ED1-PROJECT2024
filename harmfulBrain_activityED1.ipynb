{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Sponsor : Ali Ibrahim \\n\\n    Team Members :  Estuardo Melendez,  Manuel Jimenez ---- \"\"\"\"input names\\n\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#HARMFUL BRAIN ACTIVITY CLASSIFICATION PROJECT \n",
    "''' Sponsor : Ali Ibrahim \n",
    "\n",
    "    Team Members :  Estuardo Melendez,  Manuel Jimenez ---- \"\"\"\"input names\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-processing of Data :  Estuardo & Manny \n",
    "\n",
    "# Import necessary libraries for data handling, signal processing, and visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from scipy.signal import butter, filtfilt, spectrogram, iirnotch\n",
    "from mne.preprocessing import ICA\n",
    "\n",
    "#\n",
    "## IF LIBRARIES ARE THROWING AN ERROR: \n",
    "## Open terminal and run: pip install 'name_of_library', e.g., pip install pandas \n",
    "\n",
    "# ATTENTION!!!!!! : Change the directory path to your dataset\n",
    "#Manny C:/Users/Master/Documents/Project Data/hms-harmful-brain-activity-classification/train_eegs\n",
    "#Estuardo /Users/estuardomelendez/Downloads/hms-harmful-brain-activity-classification/train_eegs\n",
    "#Kevin \"C:/Users/Kevin Tran/Documents/GitHub ED1/hms-harmful-brain-activity-classificationtrain_eegs/train_eegs\"\n",
    "\n",
    "# Directory containing the training EEG files (raw data)\n",
    "train_eegs_dir = \"C:/Users/Kevin Tran/Documents/GitHub ED1/hms-harmful-brain-activity-classificationtrain_eegs/train_eegs\"\n",
    "\n",
    "\n",
    "'''\n",
    "hms-harmful-brain-activity-classification---- Data composition: \n",
    "\n",
    "\n",
    "train_eegs = Contains time-domain data\n",
    "train_spectogram = Contains frequency-domain data\n",
    "'''\n",
    "\n",
    "# List all .parquet files in the specified directory\n",
    "all_files = [os.path.join(train_eegs_dir, f) for f in os.listdir(train_eegs_dir) if f.endswith(\".parquet\")]\n",
    "\n",
    "'''\n",
    "## files_to_process can be changed to any integer between 1-100 for demo purposes or \"ALL\" for all files to be processed\n",
    "   (processing all parquet files can take a long time)\n",
    "'''\n",
    "# Allows processing of either a subset of files ex (1-100) or all files in the directory\n",
    "files_to_process = \"ALL\" # Change to \"ALL\" for processing all files\n",
    "\n",
    "# which files to be processed \n",
    "if isinstance(files_to_process, int) and 1 <= files_to_process <= 100:\n",
    "    files_to_process_list = all_files[:files_to_process]\n",
    "elif files_to_process == \"ALL\":\n",
    "    files_to_process_list = all_files\n",
    "else:\n",
    "    raise ValueError(\"files_to_process must be an integer between 1 and 100 or 'ALL'.\")\n",
    "\n",
    "# EEG Channel-to-Description Mapping: Maps channel names to their corresponding brain regions\n",
    "channel_mapping = {\n",
    "    \"Fp1\": \"Frontal pole on the left hemisphere\",\n",
    "    \"Fp2\": \"Frontal pole on the right hemisphere\",\n",
    "    \"F3\": \"Frontal lobe on the left hemisphere\",\n",
    "    \"F4\": \"Frontal lobe on the right hemisphere\",\n",
    "    \"C3\": \"Central area on the left hemisphere\",\n",
    "    \"C4\": \"Central area on the right hemisphere\",\n",
    "    \"P3\": \"Parietal lobe on the left hemisphere\",\n",
    "    \"P4\": \"Parietal lobe on the right hemisphere\",\n",
    "    \"O1\": \"Occipital lobe on the left hemisphere\",\n",
    "    \"O2\": \"Occipital lobe on the right hemisphere\",\n",
    "    \"F7\": \"Anterior temporal lobe on the left hemisphere\",\n",
    "    \"F8\": \"Anterior temporal lobe on the right hemisphere\",\n",
    "    \"T3\": \"Mid-temporal lobe on the left hemisphere\",\n",
    "    \"T4\": \"Mid-temporal lobe on the right hemisphere\",\n",
    "    \"T5\": \"Posterior temporal lobe on the left hemisphere\",\n",
    "    \"T6\": \"Posterior temporal lobe on the right hemisphere\",\n",
    "    \"Fz\": \"Frontal midline\",\n",
    "    \"Cz\": \"Central midline\",\n",
    "    \"Pz\": \"Parietal midline\"\n",
    "}\n",
    "\n",
    "# STEP 1: Notch filter function\n",
    "def apply_notch_filter(signal, fs, freq=60.0, quality_factor=2.0):\n",
    "    \n",
    "    \"\"\"\n",
    "    Removes powerline noise at a specific frequency (default 60 Hz).\n",
    "    Args:\n",
    "        signal: The EEG signal (1D array).\n",
    "        fs: Sampling frequency (Hz).\n",
    "        freq: Frequency to remove (Hz, default = 60).\n",
    "        quality_factor: Quality factor of the notch filter (default = 2.0).\n",
    "    Returns:\n",
    "        Filtered signal with the specified frequency removed.\n",
    "    \"\"\"\n",
    "\n",
    "    b, a = iirnotch(w0=freq, Q=quality_factor, fs=fs)\n",
    "    return filtfilt(b, a, signal)\n",
    "\n",
    "# STEP 2: Bandpass Filter function\n",
    "def apply_bandpass_filter(signal, fs, lowcut=0.5, highcut=40.0, order=5):\n",
    "    \n",
    "    \"\"\"\n",
    "    Retains signal components within a specific frequency range (default: 0.5–40 Hz).\n",
    "    Args:\n",
    "        signal: The EEG signal (1D array).\n",
    "        fs: Sampling frequency (Hz).\n",
    "        lowcut: Lower frequency bound (Hz, default = 0.5).\n",
    "        highcut: Upper frequency bound (Hz, default = 40).\n",
    "        order: Filter order (default = 5).\n",
    "    Returns:\n",
    "        Bandpass-filtered signal.\n",
    "    \"\"\"\n",
    "\n",
    "    nyquist = 0.5 * fs # Nyquist frequency\n",
    "    low = lowcut / nyquist\n",
    "    high = highcut / nyquist\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return filtfilt(b, a, signal)\n",
    "\n",
    "# STEP 3: Artifact Removal with ICA (Optional)\n",
    "def apply_ica(signal, fs, n_components=10):\n",
    "    \n",
    "    ica = ICA(n_components=n_components, random_state=42, max_iter='auto')\n",
    "    signal_df = pd.DataFrame(signal)  # Convert signal to DataFrame for ICA compatibility\n",
    "    ica.fit(signal_df)\n",
    "    cleaned_signal = ica.apply(signal_df)\n",
    "    return cleaned_signal.values  # Return cleaned signal as numpy array\n",
    "\n",
    "# STEP 4: Normalize Signal\n",
    "def normalize_signal(signal):\n",
    "   \n",
    "     return (signal - np.mean(signal)) / np.std(signal)\n",
    "\n",
    "# STEP 5: Spectrogram Plot with Descriptions\n",
    "def plot_spectrogram_with_description(signal, fs, channel_name, title_prefix=\"Spectrogram\"):\n",
    "    \n",
    "    #Look up the description for the channel\n",
    "    description = channel_mapping.get(channel_name, \"Unknown channel\")\n",
    "    \n",
    "    #Generate the spectrogram\n",
    "    f, t, Sxx = spectrogram(signal, fs=fs, nperseg=128)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.pcolormesh(t, f, 10 * np.log10(Sxx), shading='gouraud')\n",
    "    plt.colorbar(label='Power (dB)')\n",
    "    \n",
    "    #Add a descriptive title\n",
    "    plt.title(f\"{title_prefix} ({channel_name} - {description})\")\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Frequency (Hz)\")\n",
    "    plt.ylim(0, 100)  # Limit to 100 Hz for EEG signals\n",
    "    plt.show()\n",
    "\n",
    "# function for Time-domain Plot\n",
    "def plot_time_domain(signal, fs, channel_name, title_prefix=\"Signal in Time Domain\"):\n",
    "    \n",
    "    # Look up the description for the channel\n",
    "    description = channel_mapping.get(channel_name, \"Unknown channel\")\n",
    "    \n",
    "    time = np.arange(len(signal)) / fs\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(time, signal)\n",
    "    plt.title(f\"{title_prefix} ({channel_name} - {description})\")\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Preprocess and visualize a single File:\n",
    "def process_single_file(file_path, fs=200):\n",
    "    \n",
    "    # Load data\n",
    "    data = pd.read_parquet(file_path)\n",
    "    print(f\"Processing file: {file_path}\")\n",
    "    print(data.head())\n",
    "\n",
    "    processed_data = {}\n",
    "    for column in data.columns:\n",
    "        raw_signal = data[column].values\n",
    "        \n",
    "        #Plot raw signal spectrogram\n",
    "        plot_spectrogram_with_description(raw_signal, fs, channel_name=column, title_prefix=\"Raw Spectrogram\")\n",
    "        plot_time_domain(raw_signal, fs, channel_name=column, title_prefix=\"Raw Signal\")\n",
    "        \n",
    "        #Apply notch filter and bandpass filter\n",
    "        notch_filtered_signal = apply_notch_filter(raw_signal, fs, freq=60.0)\n",
    "        bandpass_filtered_signal = apply_bandpass_filter(notch_filtered_signal, fs, lowcut=0.5, highcut=40.0)\n",
    "        \n",
    "        # Normalize the signal\n",
    "        normalized_signal = normalize_signal(bandpass_filtered_signal)\n",
    "        \n",
    "        # (Optional) Apply ICA for artifact removal\n",
    "        # cleaned_signal = apply_ica(normalized_signal, fs)  # Uncomment if ICA is needed\n",
    "        \n",
    "        #Plot processed signal spectrogram and time-domain signal\n",
    "        plot_spectrogram_with_description(normalized_signal, fs, channel_name=column, title_prefix=\"Processed Spectrogram\")\n",
    "        plot_time_domain(normalized_signal, fs, channel_name=column, title_prefix=\"Processed Signal\")\n",
    "        \n",
    "        # Store processed data\n",
    "        processed_data[column] = normalized_signal\n",
    "\n",
    "    # Convert processed data back to DataFrame\n",
    "    processed_df = pd.DataFrame(processed_data)\n",
    "    return processed_df\n",
    "\n",
    "#Batch Process Files\n",
    "def batch_process_files(file_list, output_dir=None, fs=200):\n",
    "    \n",
    "    for file_path in file_list:\n",
    "        processed_df = process_single_file(file_path, fs=fs)\n",
    "        \n",
    "        # Optionally save the processed data\n",
    "        if output_dir:\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            output_file = os.path.join(output_dir, os.path.basename(file_path))\n",
    "            processed_df.to_parquet(output_file)\n",
    "            print(f\"Saved processed file to: {output_file}\")\n",
    "\n",
    "''' \n",
    "ATTENTION: CHANGE OUTPUT DIRECTORY F  output_dir  TO THE FILEPATH OF THE FOLDER WHERE YOU WANT TO SAVE THE PRE-PROCESSED EEGS!!!\n",
    "'''\n",
    "# Output Directory for Processed Files\n",
    "#estuardo /Users/estuardomelendez/Desktop/processed_eegs\n",
    "#manny /Users/Master/Documents/Project Data/processed_eegs\n",
    "output_dir = \"C:/Users/Kevin Tran/Documents/Project Data/processed_eegs\"  # CHANGE TO YOUR OWN DIRECTORY \n",
    "\n",
    "# Run Batch Processing with Sampling Frequency of 200 --- Can be changed according to professors instructions:\n",
    "\n",
    "batch_process_files(files_to_process_list, output_dir=output_dir, fs=200)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing Step with Parallel Processing: Kevin             Allows for quick execution of processing all 17300 files\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.signal import butter, filtfilt, iirnotch\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 📌 Define directories\n",
    "raw_data_dir = r\"C:\\Users\\Kevin Tran\\Documents\\GitHub ED1\\hms-harmful-brain-activity-classificationtrain_eegs\\train_eegs\"\n",
    "processed_data_dir = r\"C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\"\n",
    "os.makedirs(processed_data_dir, exist_ok=True)  # Ensure processed folder exists\n",
    "\n",
    "# 📌 Verify if the EEG folder exists\n",
    "if not os.path.exists(raw_data_dir):\n",
    "    raise FileNotFoundError(f\"🚨 ERROR: The directory {raw_data_dir} does not exist. Please check the path.\")\n",
    "\n",
    "# 📌 List all files to process\n",
    "all_files = [os.path.join(raw_data_dir, f) for f in os.listdir(raw_data_dir) if f.endswith(\".parquet\")]\n",
    "\n",
    "# 📌 Check if there are files to process\n",
    "if len(all_files) == 0:\n",
    "    raise FileNotFoundError(f\"🚨 ERROR: No .parquet files found in {raw_data_dir}. Please check the folder contents.\")\n",
    "\n",
    "print(f\"✅ Found {len(all_files)} .parquet files in {raw_data_dir}. Ready to process.\\n\")\n",
    "\n",
    "# 📌 Define EEG Preprocessing Functions\n",
    "def apply_notch_filter(signal, fs=200, freq=60.0, quality_factor=30):\n",
    "    \"\"\"Apply a notch filter to remove 60Hz noise.\"\"\"\n",
    "    b, a = iirnotch(w0=freq, Q=quality_factor, fs=fs)\n",
    "    return filtfilt(b, a, signal)\n",
    "\n",
    "def apply_bandpass_filter(signal, fs=200, lowcut=0.5, highcut=40.0, order=5):\n",
    "    \"\"\"Apply a bandpass filter to keep frequencies between 0.5Hz and 40Hz.\"\"\"\n",
    "    nyquist = 0.5 * fs\n",
    "    low, high = lowcut / nyquist, highcut / nyquist\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return filtfilt(b, a, signal)\n",
    "\n",
    "def normalize_signal(signal):\n",
    "    \"\"\"Normalize EEG signal to have zero mean and unit variance.\"\"\"\n",
    "    return (signal - np.mean(signal)) / np.std(signal)\n",
    "\n",
    "# 📌 Function to preprocess a single EEG file\n",
    "def process_single_file(file_path):\n",
    "    try:\n",
    "        # Load EEG Data\n",
    "        data = pd.read_parquet(file_path)\n",
    "\n",
    "        # Preprocess each EEG channel\n",
    "        processed_data = {}\n",
    "        for channel in data.columns:\n",
    "            signal = data[channel].values\n",
    "            signal = apply_notch_filter(signal)  # Remove powerline noise\n",
    "            signal = apply_bandpass_filter(signal)  # Keep only relevant frequencies\n",
    "            signal = normalize_signal(signal)  # Normalize the signal\n",
    "            processed_data[channel] = signal\n",
    "        \n",
    "        # Save the processed data\n",
    "        processed_df = pd.DataFrame(processed_data)\n",
    "        output_file = os.path.join(processed_data_dir, os.path.basename(file_path))\n",
    "        processed_df.to_parquet(output_file)\n",
    "        return f\"✅ Success: {os.path.basename(file_path)}\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"❌ Failed: {os.path.basename(file_path)} | Error: {e}\"\n",
    "\n",
    "# 📌 Threaded Batch Processing\n",
    "def batch_process_files_threading(file_list, num_workers=os.cpu_count()//4):\n",
    "    batch_size = 100  # Process files in batches of 100\n",
    "    for i in range(0, len(file_list), batch_size):\n",
    "        batch = file_list[i:i + batch_size]\n",
    "        print(f\"🚀 Processing Batch {i//batch_size+1}/{len(file_list)//batch_size+1} ({len(batch)} files)...\")\n",
    "        with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "            results = list(tqdm(executor.map(process_single_file, batch), total=len(batch), desc=f\"Batch {i//batch_size+1}\"))\n",
    "\n",
    "        # Summary of results for the current batch\n",
    "        success = [res for res in results if res.startswith(\"✅\")]\n",
    "        failed = [res for res in results if res.startswith(\"❌\")]\n",
    "        print(f\"✔️ Batch Summary: {len(success)} Success, {len(failed)} Failed\\n\")\n",
    "        if failed:\n",
    "            print(f\"❌ Failed Files: {'; '.join(failed)}\\n\")\n",
    "\n",
    "# 📌 Run Preprocessing\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"Starting threaded processing for {len(all_files)} files...\\n\")\n",
    "    batch_process_files_threading(all_files)\n",
    "    print(\"✅ All EEG files have been processed and saved.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1000913311.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1001369401.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1001487592.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1001717358.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1002136740.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1002142157.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1002197945.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1002379034.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1002576868.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\100261680.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1002858110.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1003011202.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1003163681.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1003330515.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1003353736.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1003458521.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1003517587.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1003529080.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1003675786.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1003814407.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1004019218.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1004156847.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1004534549.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1004626343.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1005166455.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1005173045.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1005200526.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1005420143.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1005941287.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1006509151.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1006575126.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1007356722.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1007506185.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1007580543.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1008031281.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1008263956.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1008331981.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1008335975.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1008426936.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1008456640.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\100865211.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1008747430.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1009006544.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1009128037.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1009215281.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\100983721.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1009851105.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1010177503.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1010304596.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1010410816.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1010452159.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1010566708.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1010760130.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1010795734.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\101080295.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1010986046.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1011074181.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1011179387.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\101122689.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1011349601.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1011778070.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\101195762.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1012577360.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1012632670.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1013100659.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1013655517.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\101366422.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1013853735.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1014084064.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1014203300.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1014814310.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1014985354.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1015063566.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1015244684.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1015395189.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1015520380.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1015533435.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\101565666.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1015861404.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1016152689.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\101619731.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1016253909.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1016412282.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1016461238.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1017383120.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1017569404.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1017659750.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1017746075.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1018200965.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1018435561.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1018864376.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1018922951.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1019088306.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1019268007.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1019308987.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1019311223.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1019408895.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1019419868.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1020397784.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1020552314.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1020554306.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\102057647.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1021052833.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1021185090.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1021439110.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1021701043.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1022751153.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1022761360.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1022769212.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1023132955.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1023846417.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1024142789.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1024159259.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1024530160.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\10249311.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1025599643.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1025655784.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1026086500.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1026359022.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1026473133.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1026567660.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1026607406.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1026895611.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1027153093.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1027353006.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1027653863.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1028067738.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1028245680.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1028528611.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1028978272.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1029122980.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1029773755.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1029863597.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1029876643.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1029966334.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1030044632.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1030270485.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1030375965.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1030387714.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1030480798.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1030673804.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\103069834.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1031253126.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1031266155.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1031430408.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1031588198.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1031788039.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1032095383.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1032472598.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1032644496.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1032945224.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1033152642.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1033352235.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1033402741.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1033477529.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1033487223.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1033678548.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1033688929.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1033711051.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\103375107.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1033944434.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1034153614.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1034222828.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\10343849.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1035494544.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1035710827.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1035875156.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1036203710.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1036339625.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1036506711.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1036581668.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1036619208.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1036922700.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1037053695.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1037432925.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1037555924.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1037773381.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1038108573.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1038151191.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1038522231.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\10386542.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1038675800.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1038740363.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1039058822.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\103921849.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1039229136.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1039503382.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1039933132.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1039968552.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1039994266.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1040654873.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1040792873.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1041025912.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1041252567.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\104130425.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\104136958.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1041497679.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1041541339.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1042517720.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1042732873.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1042819650.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1043004381.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1043874234.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1044076067.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1044649692.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1044863512.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1044932137.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1045429553.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1045723955.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1045793518.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1045895032.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1046303268.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1046614618.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\10466156.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1046635662.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1046792525.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1046955522.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1047041162.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1047344237.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1047471605.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1047576492.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1047741956.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\104817853.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1048681252.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1048893097.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1049314824.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1049329959.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1049419949.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\104950672.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1049785745.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1049822368.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1050317502.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1050422172.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1050474295.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1050794646.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1050927780.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1050997411.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1051209118.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1051718046.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1052025587.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1052075017.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1052137713.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1052181360.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1052525377.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1052555553.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1052595853.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1052788311.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1052897805.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1053473655.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1053848474.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\105417608.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1054182773.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1054252441.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1054776749.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1054815914.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1054846702.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1054942570.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1055172672.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1055532574.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1055553016.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1056078703.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1056283163.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1057127271.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1057446731.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1057623030.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1057661588.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1057856283.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1057892054.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1057959045.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1058037294.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1058058150.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1058173302.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1058503230.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1058522598.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1058584796.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1059079814.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1059080911.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1059149892.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1059944827.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1060241087.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1060315464.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1060486426.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\106068696.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1061139600.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\106121624.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\106141507.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\106160943.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\10617205.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1062096035.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1062215420.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1062759050.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1064261721.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1064295003.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1064331306.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1064439359.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\106456230.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1064756905.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1065023048.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1065039525.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1065173994.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1065281847.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1065375820.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1065431763.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1065775016.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1066355107.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1066511099.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1066845975.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1066942251.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1067056536.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\106747872.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1067973358.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1068199303.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1068594966.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\10687514.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1068977348.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1069529026.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1069548217.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1069700335.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1069815064.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1070713177.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1070944390.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1070965297.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1071324276.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1071587473.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1071616064.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1071831200.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1071869394.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1071881837.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1072078183.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1072232739.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1072523798.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1072707945.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1072808361.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1073299938.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1073419985.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\107362020.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1073656815.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1073878025.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1073960406.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1074584901.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1075088596.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1075247364.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1075442225.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1075672255.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1075725552.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1075963156.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1076057868.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1076260800.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1076316607.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1076434509.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1076641674.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\107701457.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1077418506.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1077972767.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1078232939.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1078429832.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1079201033.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1079450443.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1079646978.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1079662656.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1080450640.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1080752861.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1080879993.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1081012860.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1081228840.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1081437630.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1081492682.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1081585784.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1081824509.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1082211003.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1082987034.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1083405365.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1083464967.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\108347476.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1083753893.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1084922224.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1085206902.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1085276876.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1085571772.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1085623032.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1085664125.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1085670072.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1085698914.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1085833294.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\10859011.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1086250795.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1086780865.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1086941408.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1087184899.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1087457964.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1087788519.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1087866811.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\108795029.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1088119333.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\108824720.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1088509296.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1088537475.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1088554081.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1088716964.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1089097453.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1089780080.parquet\n",
      "Extracting features from: C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\\1090151176.parquet\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 101\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_path \u001b[38;5;129;01min\u001b[39;00m processed_files:\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracting features from: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 101\u001b[0m     file_features \u001b[38;5;241m=\u001b[39m \u001b[43mextract_features_from_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_size_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m     combined_features\u001b[38;5;241m.\u001b[39mappend(file_features)\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m# Combine features from all files into a single DataFrame\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 83\u001b[0m, in \u001b[0;36mextract_features_from_file\u001b[1;34m(file_path, fs, window_size_samples)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, window \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(windows):\n\u001b[0;32m     82\u001b[0m     time_features \u001b[38;5;241m=\u001b[39m extract_time_features(window)\n\u001b[1;32m---> 83\u001b[0m     frequency_features \u001b[38;5;241m=\u001b[39m \u001b[43mextract_frequency_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m     combined_features \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m: os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(file_path),\n\u001b[0;32m     86\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchannel\u001b[39m\u001b[38;5;124m\"\u001b[39m: channel,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfrequency_features\n\u001b[0;32m     90\u001b[0m     }\n\u001b[0;32m     91\u001b[0m     all_features\u001b[38;5;241m.\u001b[39mappend(combined_features)\n",
      "Cell \u001b[1;32mIn[5], line 55\u001b[0m, in \u001b[0;36mextract_frequency_features\u001b[1;34m(signal, fs)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_frequency_features\u001b[39m(signal, fs):\n\u001b[1;32m---> 55\u001b[0m     freqs, psd \u001b[38;5;241m=\u001b[39m \u001b[43mwelch\u001b[49m\u001b[43m(\u001b[49m\u001b[43msignal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnperseg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#Compute PSD using Welch's method\u001b[39;00m\n\u001b[0;32m     56\u001b[0m     band_powers \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     57\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelta_power\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39msum(psd[(freqs \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m) \u001b[38;5;241m&\u001b[39m (freqs \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m4\u001b[39m)]),\n\u001b[0;32m     58\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtheta_power\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39msum(psd[(freqs \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m) \u001b[38;5;241m&\u001b[39m (freqs \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m8\u001b[39m)]),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspectral_entropy\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39msum(psd \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(psd \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-10\u001b[39m)) \u001b[38;5;66;03m## Avoid log(0) with small offset\u001b[39;00m\n\u001b[0;32m     63\u001b[0m     }\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m band_powers\n",
      "File \u001b[1;32mc:\\Users\\Kevin Tran\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\signal\\_spectral_py.py:462\u001b[0m, in \u001b[0;36mwelch\u001b[1;34m(x, fs, window, nperseg, noverlap, nfft, detrend, return_onesided, scaling, axis, average)\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwelch\u001b[39m(x, fs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, window\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhann\u001b[39m\u001b[38;5;124m'\u001b[39m, nperseg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, noverlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, nfft\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    301\u001b[0m           detrend\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m'\u001b[39m, return_onesided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, scaling\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdensity\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    302\u001b[0m           axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    303\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;124;03m    Estimate power spectral density using Welch's method.\u001b[39;00m\n\u001b[0;32m    305\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    460\u001b[0m \n\u001b[0;32m    461\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 462\u001b[0m     freqs, Pxx \u001b[38;5;241m=\u001b[39m \u001b[43mcsd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnperseg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnperseg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    463\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mnoverlap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnoverlap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnfft\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnfft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetrend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetrend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    464\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mreturn_onesided\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_onesided\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[43m                     \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    467\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m freqs, Pxx\u001b[38;5;241m.\u001b[39mreal\n",
      "File \u001b[1;32mc:\\Users\\Kevin Tran\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\signal\\_spectral_py.py:600\u001b[0m, in \u001b[0;36mcsd\u001b[1;34m(x, y, fs, window, nperseg, noverlap, nfft, detrend, return_onesided, scaling, axis, average)\u001b[0m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcsd\u001b[39m(x, y, fs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, window\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhann\u001b[39m\u001b[38;5;124m'\u001b[39m, nperseg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, noverlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, nfft\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    471\u001b[0m         detrend\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m'\u001b[39m, return_onesided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, scaling\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdensity\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    472\u001b[0m         axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    473\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;124;03m    Estimate the cross power spectral density, Pxy, using Welch's method.\u001b[39;00m\n\u001b[0;32m    475\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    598\u001b[0m \n\u001b[0;32m    599\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 600\u001b[0m     freqs, _, Pxy \u001b[38;5;241m=\u001b[39m \u001b[43m_spectral_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnperseg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoverlap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    601\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mnfft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetrend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_onesided\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    602\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpsd\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    604\u001b[0m     \u001b[38;5;66;03m# Average over windows.\u001b[39;00m\n\u001b[0;32m    605\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(Pxy\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m Pxy\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Kevin Tran\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\signal\\_spectral_py.py:1928\u001b[0m, in \u001b[0;36m_spectral_helper\u001b[1;34m(x, y, fs, window, nperseg, noverlap, nfft, detrend, return_onesided, scaling, axis, mode, boundary, padded)\u001b[0m\n\u001b[0;32m   1925\u001b[0m     freqs \u001b[38;5;241m=\u001b[39m sp_fft\u001b[38;5;241m.\u001b[39mrfftfreq(nfft, \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39mfs)\n\u001b[0;32m   1927\u001b[0m \u001b[38;5;66;03m# Perform the windowed FFTs\u001b[39;00m\n\u001b[1;32m-> 1928\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43m_fft_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetrend_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnperseg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoverlap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnfft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msides\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1930\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m same_data:\n\u001b[0;32m   1931\u001b[0m     \u001b[38;5;66;03m# All the same operations on the y data\u001b[39;00m\n\u001b[0;32m   1932\u001b[0m     result_y \u001b[38;5;241m=\u001b[39m _fft_helper(y, win, detrend_func, nperseg, noverlap, nfft,\n\u001b[0;32m   1933\u001b[0m                            sides)\n",
      "File \u001b[1;32mc:\\Users\\Kevin Tran\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\signal\\_spectral_py.py:2001\u001b[0m, in \u001b[0;36m_fft_helper\u001b[1;34m(x, win, detrend_func, nperseg, noverlap, nfft, sides)\u001b[0m\n\u001b[0;32m   1998\u001b[0m     result \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m0\u001b[39m::step, :]\n\u001b[0;32m   2000\u001b[0m \u001b[38;5;66;03m# Detrend each data segment individually\u001b[39;00m\n\u001b[1;32m-> 2001\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mdetrend_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# Apply window by multiplication\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m result \u001b[38;5;241m=\u001b[39m win \u001b[38;5;241m*\u001b[39m result\n",
      "File \u001b[1;32mc:\\Users\\Kevin Tran\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\signal\\_spectral_py.py:1882\u001b[0m, in \u001b[0;36m_spectral_helper.<locals>.detrend_func\u001b[1;34m(d)\u001b[0m\n\u001b[0;32m   1881\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdetrend_func\u001b[39m(d):\n\u001b[1;32m-> 1882\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_signaltools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetrend\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetrend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#FEATURE EXTRACTION: Estuardo & Manny \n",
    "'''\n",
    "Preliminary implementation for feature extraction, We divided the parquet eeg file into 2 second intervals\n",
    "and extracted time domain features such as :[mean, variance, skewness, kurtosis, rms, delta power, theta power ,alpha power, beta power & gamma]\n",
    "\n",
    "Me & Manny are still waiting on our Sposors feedback, you will find a eeg_features file which I recommend you open with the RAINBOW CSV extension\n",
    "for visual studio.\n",
    "\n",
    "#Remember to change file paths to your own dataset directory and comment out the previos one\n",
    "\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.signal import welch\n",
    "import os\n",
    "\n",
    "# CHANGE DIRECTORY TO YOU PROCESSED EEGS!!!!!\n",
    "#Estuardo \"/Users/estuardomelendez/Desktop/processed_eegs\"\n",
    "#Manny \"C:/Users/Master/Documents/GitHub/ED1-PROJECT2024/processed_eegs\" \n",
    "processed_eegs_dir = r\"C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\"\n",
    "\n",
    "processed_files = [os.path.join(processed_eegs_dir, f) for f in os.listdir(processed_eegs_dir) if f.endswith(\".parquet\")]\n",
    "\n",
    "\n",
    "# Sampling frequency (Hz) and window size (in seconds)\n",
    "fs = 200  # Sampling frequency (200 samples per second)\n",
    "window_size_seconds = 2  # Duration of each window in seconds\n",
    "window_size_samples = fs * window_size_seconds  # Convert window size to samples\n",
    "\n",
    "#Function to split data into windows\n",
    "def split_into_windows(signal, window_size):\n",
    "    \n",
    "    num_windows = len(signal) // window_size # Calculate number of complete windows\n",
    "    windows = [signal[i * window_size:(i + 1) * window_size] for i in range(num_windows)]\n",
    "    return windows\n",
    "\n",
    "#Function to extract time-domain features\n",
    "def extract_time_features(signal):\n",
    "    \n",
    "    features = {\n",
    "        \"mean\": np.mean(signal),\n",
    "        \"variance\": np.var(signal),\n",
    "        \"skewness\": skew(signal),\n",
    "        \"kurtosis\": kurtosis(signal),\n",
    "        \"rms\": np.sqrt(np.mean(signal**2)),\n",
    "        \"zero_crossing_rate\": np.sum(np.diff(np.sign(signal)) != 0) / len(signal)\n",
    "    }\n",
    "    return features\n",
    "\n",
    "#function to extract frequency-domain features\n",
    "def extract_frequency_features(signal, fs):\n",
    "    \n",
    "    freqs, psd = welch(signal, fs=fs, nperseg=256) #Compute PSD using Welch's method\n",
    "    band_powers = {\n",
    "        \"delta_power\": np.sum(psd[(freqs >= 0.5) & (freqs < 4)]),\n",
    "        \"theta_power\": np.sum(psd[(freqs >= 4) & (freqs < 8)]),\n",
    "        \"alpha_power\": np.sum(psd[(freqs >= 8) & (freqs < 13)]),\n",
    "        \"beta_power\": np.sum(psd[(freqs >= 13) & (freqs < 30)]),\n",
    "        \"gamma_power\": np.sum(psd[(freqs >= 30) & (freqs < 50)]),\n",
    "        \"spectral_entropy\": -np.sum(psd * np.log(psd + 1e-10)) ## Avoid log(0) with small offset\n",
    "    }\n",
    "    return band_powers\n",
    "\n",
    "#function to extract features from a single EEG file\n",
    "def extract_features_from_file(file_path, fs, window_size_samples):\n",
    "   \n",
    "    # Load processed EEG data\n",
    "    data = pd.read_parquet(file_path)\n",
    "    \n",
    "    # Extract features for each channel\n",
    "    all_features = []\n",
    "    for channel in data.columns:\n",
    "        signal = data[channel].values\n",
    "        \n",
    "        # Split the signal into windows\n",
    "        windows = split_into_windows(signal, window_size_samples)\n",
    "        \n",
    "        # Extract features from each window\n",
    "        for i, window in enumerate(windows):\n",
    "            time_features = extract_time_features(window)\n",
    "            frequency_features = extract_frequency_features(window, fs)\n",
    "            combined_features = {\n",
    "                \"file\": os.path.basename(file_path),\n",
    "                \"channel\": channel,\n",
    "                \"window\": i,\n",
    "                **time_features,\n",
    "                **frequency_features\n",
    "            }\n",
    "            all_features.append(combined_features)\n",
    "    \n",
    "    # Return DataFrame with properly named columns\n",
    "    return pd.DataFrame(all_features)\n",
    "\n",
    "# Process all files and combine features by extracting features from all files in the directory\n",
    "combined_features = []\n",
    "\n",
    "for file_path in processed_files:\n",
    "    print(f\"Extracting features from: {file_path}\")\n",
    "    file_features = extract_features_from_file(file_path, fs, window_size_samples)\n",
    "    combined_features.append(file_features)\n",
    "\n",
    "# Combine features from all files into a single DataFrame\n",
    "all_features_df = pd.concat(combined_features, ignore_index=True)\n",
    "\n",
    "# Save extracted features to a CSV file for later use\n",
    "#Estuardo \"/Users/estuardomelendez/Desktop/eeg_features.csv\"\n",
    "#Manny /Users/Master/Documents/GitHub/ED1-PROJECT2024/eeg_features.csv\n",
    "output_feature_file = \"C:/Users/Kevin Tran/Documents/Project Data/feature extractions/eeg_features.csv\"\n",
    "all_features_df.to_csv(output_feature_file, index=False)\n",
    "\n",
    "# Preview the extracted features\n",
    "print(f\"Features saved to: {output_feature_file}\")\n",
    "print(\"\\nPreview of Extracted Features:\")\n",
    "print(all_features_df.head())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting feature extraction for 17300 processed EEG files...\n",
      "\n",
      "🚀 Processing Batch 1/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 1: 100%|██████████| 100/100 [01:29<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 2/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 2: 100%|██████████| 100/100 [01:20<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 3/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 3: 100%|██████████| 100/100 [01:32<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 4/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 4: 100%|██████████| 100/100 [01:14<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 5/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 5: 100%|██████████| 100/100 [01:15<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 6/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 6: 100%|██████████| 100/100 [01:37<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 7/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 7: 100%|██████████| 100/100 [01:18<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 8/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 8: 100%|██████████| 100/100 [01:13<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 9/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 9: 100%|██████████| 100/100 [01:12<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 10/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 10: 100%|██████████| 100/100 [01:19<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 11/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 11: 100%|██████████| 100/100 [01:19<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 12/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 12: 100%|██████████| 100/100 [01:12<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 13/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 13: 100%|██████████| 100/100 [01:14<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 14/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 14: 100%|██████████| 100/100 [01:16<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 15/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 15: 100%|██████████| 100/100 [01:21<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 16/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 16: 100%|██████████| 100/100 [01:17<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 17/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 17: 100%|██████████| 100/100 [01:21<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 18/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 18: 100%|██████████| 100/100 [01:18<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 19/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 19: 100%|██████████| 100/100 [01:35<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 20/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 20: 100%|██████████| 100/100 [01:10<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 21/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 21: 100%|██████████| 100/100 [01:30<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 22/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 22: 100%|██████████| 100/100 [01:38<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 23/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 23: 100%|██████████| 100/100 [01:13<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 24/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 24: 100%|██████████| 100/100 [01:11<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 25/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 25: 100%|██████████| 100/100 [01:11<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 26/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 26: 100%|██████████| 100/100 [01:17<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 27/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 27: 100%|██████████| 100/100 [01:14<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 28/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 28: 100%|██████████| 100/100 [01:25<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 29/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 29: 100%|██████████| 100/100 [01:07<00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 30/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 30: 100%|██████████| 100/100 [01:43<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 31/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 31: 100%|██████████| 100/100 [01:21<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 32/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 32: 100%|██████████| 100/100 [01:07<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 33/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 33: 100%|██████████| 100/100 [01:25<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 34/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 34: 100%|██████████| 100/100 [01:15<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 35/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 35: 100%|██████████| 100/100 [01:13<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 36/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 36: 100%|██████████| 100/100 [01:18<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 37/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 37: 100%|██████████| 100/100 [01:14<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 38/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 38: 100%|██████████| 100/100 [01:12<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 39/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 39: 100%|██████████| 100/100 [01:13<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 40/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 40: 100%|██████████| 100/100 [01:34<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 41/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 41: 100%|██████████| 100/100 [01:16<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 42/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 42: 100%|██████████| 100/100 [01:24<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 43/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 43: 100%|██████████| 100/100 [01:04<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 44/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 44: 100%|██████████| 100/100 [01:17<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 45/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 45: 100%|██████████| 100/100 [01:15<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 46/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 46: 100%|██████████| 100/100 [01:13<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 47/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 47: 100%|██████████| 100/100 [01:15<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 48/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 48: 100%|██████████| 100/100 [01:20<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 49/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 49: 100%|██████████| 100/100 [01:10<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 50/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 50: 100%|██████████| 100/100 [01:24<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 51/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 51: 100%|██████████| 100/100 [01:09<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 52/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 52: 100%|██████████| 100/100 [01:08<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 53/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 53: 100%|██████████| 100/100 [01:24<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 54/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 54: 100%|██████████| 100/100 [01:11<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 55/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 55: 100%|██████████| 100/100 [01:22<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 56/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 56: 100%|██████████| 100/100 [01:13<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 57/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 57: 100%|██████████| 100/100 [01:40<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 58/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 58: 100%|██████████| 100/100 [01:06<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 59/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 59: 100%|██████████| 100/100 [01:18<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 60/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 60: 100%|██████████| 100/100 [01:04<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 61/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 61: 100%|██████████| 100/100 [01:13<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 62/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 62: 100%|██████████| 100/100 [01:12<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 63/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 63: 100%|██████████| 100/100 [01:14<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 64/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 64: 100%|██████████| 100/100 [01:11<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 65/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 65: 100%|██████████| 100/100 [01:34<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 66/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 66: 100%|██████████| 100/100 [01:09<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 67/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 67: 100%|██████████| 100/100 [01:12<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 68/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 68: 100%|██████████| 100/100 [01:10<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 69/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 69: 100%|██████████| 100/100 [01:18<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 70/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 70: 100%|██████████| 100/100 [01:14<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 71/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 71: 100%|██████████| 100/100 [01:09<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 72/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 72: 100%|██████████| 100/100 [01:06<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 73/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 73: 100%|██████████| 100/100 [01:10<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 74/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 74: 100%|██████████| 100/100 [01:13<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 75/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 75: 100%|██████████| 100/100 [01:18<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 76/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 76: 100%|██████████| 100/100 [01:08<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 77/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 77: 100%|██████████| 100/100 [01:17<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 78/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 78: 100%|██████████| 100/100 [01:11<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 79/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 79: 100%|██████████| 100/100 [01:14<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 80/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 80: 100%|██████████| 100/100 [01:12<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 81/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 81: 100%|██████████| 100/100 [01:13<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 82/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 82: 100%|██████████| 100/100 [01:08<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 83/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 83: 100%|██████████| 100/100 [01:10<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 84/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 84:  49%|████▉     | 49/100 [00:40<00:56,  1.10s/it]C:\\Users\\Kevin Tran\\AppData\\Local\\Temp\\ipykernel_10848\\3894915797.py:33: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  \"skewness\": skew(signal),\n",
      "C:\\Users\\Kevin Tran\\AppData\\Local\\Temp\\ipykernel_10848\\3894915797.py:34: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  \"kurtosis\": kurtosis(signal),\n",
      "Batch 84: 100%|██████████| 100/100 [01:39<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 85/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 85: 100%|██████████| 100/100 [01:12<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 86/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 86: 100%|██████████| 100/100 [01:17<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 87/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 87: 100%|██████████| 100/100 [01:09<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 88/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 88: 100%|██████████| 100/100 [01:08<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 89/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 89: 100%|██████████| 100/100 [01:29<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 90/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 90: 100%|██████████| 100/100 [01:15<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 91/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 91: 100%|██████████| 100/100 [01:08<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 92/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 92: 100%|██████████| 100/100 [01:05<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 93/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 93: 100%|██████████| 100/100 [01:06<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 94/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 94: 100%|██████████| 100/100 [01:12<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 95/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 95: 100%|██████████| 100/100 [01:16<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 96/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 96: 100%|██████████| 100/100 [01:19<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 97/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 97: 100%|██████████| 100/100 [01:13<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 98/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 98: 100%|██████████| 100/100 [01:13<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 99/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 99: 100%|██████████| 100/100 [01:11<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 100/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 100: 100%|██████████| 100/100 [01:13<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 101/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 101: 100%|██████████| 100/100 [01:24<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 102/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 102: 100%|██████████| 100/100 [01:11<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 103/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 103: 100%|██████████| 100/100 [01:19<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 104/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 104: 100%|██████████| 100/100 [01:12<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 105/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 105: 100%|██████████| 100/100 [01:18<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 106/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 106: 100%|██████████| 100/100 [01:13<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 107/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 107: 100%|██████████| 100/100 [01:16<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 108/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 108: 100%|██████████| 100/100 [01:14<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 109/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 109: 100%|██████████| 100/100 [01:06<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 110/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 110: 100%|██████████| 100/100 [01:11<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 111/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 111: 100%|██████████| 100/100 [01:17<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 112/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 112: 100%|██████████| 100/100 [01:12<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 113/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 113: 100%|██████████| 100/100 [01:19<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 114/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 114: 100%|██████████| 100/100 [01:11<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 115/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 115: 100%|██████████| 100/100 [01:15<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 116/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 116: 100%|██████████| 100/100 [01:17<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 117/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 117: 100%|██████████| 100/100 [01:08<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 118/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 118: 100%|██████████| 100/100 [01:15<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 119/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 119: 100%|██████████| 100/100 [01:09<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 120/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 120: 100%|██████████| 100/100 [01:17<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 121/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 121: 100%|██████████| 100/100 [01:29<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 122/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 122: 100%|██████████| 100/100 [01:21<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 123/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 123: 100%|██████████| 100/100 [01:26<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 124/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 124: 100%|██████████| 100/100 [01:08<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 125/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 125: 100%|██████████| 100/100 [01:13<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 126/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 126: 100%|██████████| 100/100 [01:10<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 127/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 127: 100%|██████████| 100/100 [01:10<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 128/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 128: 100%|██████████| 100/100 [01:08<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 129/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 129: 100%|██████████| 100/100 [01:02<00:00,  1.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 130/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 130: 100%|██████████| 100/100 [01:10<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 131/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 131: 100%|██████████| 100/100 [01:08<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 132/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 132: 100%|██████████| 100/100 [01:12<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 133/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 133: 100%|██████████| 100/100 [01:13<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 134/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 134: 100%|██████████| 100/100 [01:04<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 135/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 135: 100%|██████████| 100/100 [01:18<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 136/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 136: 100%|██████████| 100/100 [01:15<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 137/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 137: 100%|██████████| 100/100 [01:21<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 138/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 138: 100%|██████████| 100/100 [01:21<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 139/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 139: 100%|██████████| 100/100 [01:15<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 140/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 140: 100%|██████████| 100/100 [01:07<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 141/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 141: 100%|██████████| 100/100 [01:19<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 142/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 142: 100%|██████████| 100/100 [01:13<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 143/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 143: 100%|██████████| 100/100 [01:13<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 144/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 144: 100%|██████████| 100/100 [01:17<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 145/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 145: 100%|██████████| 100/100 [01:26<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 146/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 146: 100%|██████████| 100/100 [01:09<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 147/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 147: 100%|██████████| 100/100 [01:08<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 148/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 148: 100%|██████████| 100/100 [01:10<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 149/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 149: 100%|██████████| 100/100 [01:16<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 150/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 150: 100%|██████████| 100/100 [01:17<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 151/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 151: 100%|██████████| 100/100 [01:15<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 152/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 152: 100%|██████████| 100/100 [01:32<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 153/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 153: 100%|██████████| 100/100 [01:20<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 154/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 154: 100%|██████████| 100/100 [01:21<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 155/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 155: 100%|██████████| 100/100 [01:25<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 156/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 156: 100%|██████████| 100/100 [01:15<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 157/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 157: 100%|██████████| 100/100 [01:15<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 158/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 158: 100%|██████████| 100/100 [01:13<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 159/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 159: 100%|██████████| 100/100 [01:07<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 160/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 160: 100%|██████████| 100/100 [01:34<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 161/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 161: 100%|██████████| 100/100 [01:24<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 162/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 162: 100%|██████████| 100/100 [01:23<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 163/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 163: 100%|██████████| 100/100 [01:15<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 164/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 164: 100%|██████████| 100/100 [01:15<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 165/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 165: 100%|██████████| 100/100 [01:13<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 166/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 166: 100%|██████████| 100/100 [01:35<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 167/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 167: 100%|██████████| 100/100 [01:13<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 168/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 168: 100%|██████████| 100/100 [01:06<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 169/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 169: 100%|██████████| 100/100 [01:11<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 170/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 170: 100%|██████████| 100/100 [01:17<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 171/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 171: 100%|██████████| 100/100 [01:15<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 172/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 172: 100%|██████████| 100/100 [01:18<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing Batch 173/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 173: 100%|██████████| 100/100 [01:12<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Feature extraction completed. Results saved to: C:\\Users\\Kevin Tran\\Documents\\Project Data\\feature extractions\\eeg_features.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Feature Extraction with Parallel Processing\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.signal import welch\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 📌 Directories\n",
    "processed_eegs_dir = r\"C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\"\n",
    "output_feature_file = r\"C:\\Users\\Kevin Tran\\Documents\\Project Data\\feature extractions\\eeg_features.csv\"\n",
    "os.makedirs(os.path.dirname(output_feature_file), exist_ok=True)  # Ensure output folder exists\n",
    "\n",
    "processed_files = [os.path.join(processed_eegs_dir, f) for f in os.listdir(processed_eegs_dir) if f.endswith(\".parquet\")]\n",
    "\n",
    "# Sampling frequency (Hz) and window size (in seconds)\n",
    "fs = 200  # Sampling frequency\n",
    "window_size_seconds = 2  # Duration of each window in seconds\n",
    "window_size_samples = fs * window_size_seconds  # Convert window size to samples\n",
    "\n",
    "# Function to split data into windows\n",
    "def split_into_windows(signal, window_size):\n",
    "    num_windows = len(signal) // window_size\n",
    "    return [signal[i * window_size:(i + 1) * window_size] for i in range(num_windows)]\n",
    "\n",
    "# Function to extract time-domain features\n",
    "def extract_time_features(signal):\n",
    "    return {\n",
    "        \"mean\": np.mean(signal),\n",
    "        \"variance\": np.var(signal),\n",
    "        \"skewness\": skew(signal),\n",
    "        \"kurtosis\": kurtosis(signal),\n",
    "        \"rms\": np.sqrt(np.mean(signal**2)),\n",
    "        \"zero_crossing_rate\": np.sum(np.diff(np.sign(signal)) != 0) / len(signal),\n",
    "    }\n",
    "\n",
    "# Function to extract frequency-domain features\n",
    "def extract_frequency_features(signal, fs):\n",
    "    freqs, psd = welch(signal, fs=fs, nperseg=256)\n",
    "    return {\n",
    "        \"delta_power\": np.sum(psd[(freqs >= 0.5) & (freqs < 4)]),\n",
    "        \"theta_power\": np.sum(psd[(freqs >= 4) & (freqs < 8)]),\n",
    "        \"alpha_power\": np.sum(psd[(freqs >= 8) & (freqs < 13)]),\n",
    "        \"beta_power\": np.sum(psd[(freqs >= 13) & (freqs < 30)]),\n",
    "        \"gamma_power\": np.sum(psd[(freqs >= 30) & (freqs < 50)]),\n",
    "        \"spectral_entropy\": -np.sum(psd * np.log(psd + 1e-10)),  # Avoid log(0)\n",
    "    }\n",
    "\n",
    "# Function to extract features from a single EEG file\n",
    "def extract_features_from_file(file_path):\n",
    "    try:\n",
    "        data = pd.read_parquet(file_path)\n",
    "        all_features = []\n",
    "\n",
    "        for channel in data.columns:\n",
    "            signal = data[channel].values\n",
    "            windows = split_into_windows(signal, window_size_samples)\n",
    "\n",
    "            for i, window in enumerate(windows):\n",
    "                time_features = extract_time_features(window)\n",
    "                frequency_features = extract_frequency_features(window, fs)\n",
    "                combined_features = {\n",
    "                    \"file\": os.path.basename(file_path),\n",
    "                    \"channel\": channel,\n",
    "                    \"window\": i,\n",
    "                    **time_features,\n",
    "                    **frequency_features,\n",
    "                }\n",
    "                all_features.append(combined_features)\n",
    "\n",
    "        return pd.DataFrame(all_features)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing {file_path}: {e}\")\n",
    "        return pd.DataFrame()  # Return an empty DataFrame on failure\n",
    "\n",
    "# Function for threaded batch processing\n",
    "def batch_extract_features(file_list, num_workers=os.cpu_count()//4):\n",
    "    batch_size = 100  # Process files in batches of 100\n",
    "    combined_features = []\n",
    "\n",
    "    for i in range(0, len(file_list), batch_size):\n",
    "        batch = file_list[i:i + batch_size]\n",
    "        print(f\"🚀 Processing Batch {i//batch_size+1}/{len(file_list)//batch_size+1} ({len(batch)} files)...\")\n",
    "        with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "            results = list(tqdm(executor.map(extract_features_from_file, batch), total=len(batch), desc=f\"Batch {i//batch_size+1}\"))\n",
    "\n",
    "        # Combine features from the batch\n",
    "        for df in results:\n",
    "            if not df.empty:\n",
    "                combined_features.append(df)\n",
    "\n",
    "    return pd.concat(combined_features, ignore_index=True)\n",
    "\n",
    "# 📌 Run Feature Extraction\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"Starting feature extraction for {len(processed_files)} processed EEG files...\\n\")\n",
    "    all_features_df = batch_extract_features(processed_files)\n",
    "    all_features_df.to_csv(output_feature_file, index=False)\n",
    "    print(f\"✅ Feature extraction completed. Results saved to: {output_feature_file}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting feature extraction for 17300 processed EEG files...\n",
      "\n",
      "🚀 Processing Batch 1/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 1: 100%|██████████| 100/100 [01:32<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️ Batch Summary: 100 Success, 0 Failed\n",
      "\n",
      "🚀 Processing Batch 2/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 2: 100%|██████████| 100/100 [01:22<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️ Batch Summary: 100 Success, 0 Failed\n",
      "\n",
      "🚀 Processing Batch 3/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 3: 100%|██████████| 100/100 [01:35<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️ Batch Summary: 100 Success, 0 Failed\n",
      "\n",
      "🚀 Processing Batch 4/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 4: 100%|██████████| 100/100 [01:15<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️ Batch Summary: 100 Success, 0 Failed\n",
      "\n",
      "🚀 Processing Batch 5/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 5: 100%|██████████| 100/100 [01:17<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️ Batch Summary: 100 Success, 0 Failed\n",
      "\n",
      "🚀 Processing Batch 6/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 6: 100%|██████████| 100/100 [01:45<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️ Batch Summary: 100 Success, 0 Failed\n",
      "\n",
      "🚀 Processing Batch 7/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 7: 100%|██████████| 100/100 [01:26<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️ Batch Summary: 100 Success, 0 Failed\n",
      "\n",
      "🚀 Processing Batch 8/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 8: 100%|██████████| 100/100 [01:18<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️ Batch Summary: 100 Success, 0 Failed\n",
      "\n",
      "🚀 Processing Batch 9/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 9: 100%|██████████| 100/100 [01:16<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️ Batch Summary: 100 Success, 0 Failed\n",
      "\n",
      "🚀 Processing Batch 10/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 10: 100%|██████████| 100/100 [01:21<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️ Batch Summary: 100 Success, 0 Failed\n",
      "\n",
      "🚀 Processing Batch 11/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 11: 100%|██████████| 100/100 [01:19<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️ Batch Summary: 100 Success, 0 Failed\n",
      "\n",
      "🚀 Processing Batch 12/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 12: 100%|██████████| 100/100 [01:13<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️ Batch Summary: 100 Success, 0 Failed\n",
      "\n",
      "🚀 Processing Batch 13/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 13: 100%|██████████| 100/100 [01:12<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️ Batch Summary: 100 Success, 0 Failed\n",
      "\n",
      "🚀 Processing Batch 14/174 (100 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 14:  76%|███████▌  | 76/100 [01:01<00:19,  1.24it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 125\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting feature extraction for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(processed_files)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m processed EEG files...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 125\u001b[0m     \u001b[43mbatch_extract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_files\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Feature extraction completed. Temporary files saved to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_feature_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;66;03m# Merge temporary files and clean up\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[7], line 89\u001b[0m, in \u001b[0;36mbatch_extract_features\u001b[1;34m(file_list, num_workers)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🚀 Processing Batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mbatch_size\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(file_list)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mbatch_size\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(batch)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m files)...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ThreadPoolExecutor(max_workers\u001b[38;5;241m=\u001b[39mnum_workers) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m---> 89\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecutor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextract_features_from_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBatch \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# Print summary of results for the current batch\u001b[39;00m\n\u001b[0;32m     92\u001b[0m success \u001b[38;5;241m=\u001b[39m [res \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results \u001b[38;5;28;01mif\u001b[39;00m res\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n",
      "File \u001b[1;32mc:\\Users\\Kevin Tran\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1192\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1197\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1198\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Kevin Tran\\AppData\\Local\\Programs\\Python\\Python310\\lib\\concurrent\\futures\\_base.py:609\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[1;34m()\u001b[0m\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[0;32m    607\u001b[0m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 609\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    610\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    611\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m fs\u001b[38;5;241m.\u001b[39mpop()\u001b[38;5;241m.\u001b[39mresult(end_time \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic())\n",
      "File \u001b[1;32mc:\\Users\\Kevin Tran\\AppData\\Local\\Programs\\Python\\Python310\\lib\\concurrent\\futures\\_base.py:441\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m    439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[1;32m--> 441\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[0;32m    444\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[1;32mc:\\Users\\Kevin Tran\\AppData\\Local\\Programs\\Python\\Python310\\lib\\threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Feature extraction 2 test\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.signal import welch\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 📌 Directories\n",
    "processed_eegs_dir = r\"C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\"\n",
    "output_feature_dir = r\"C:\\Users\\Kevin Tran\\Documents\\Project Data\\feature extractions\\temp\"\n",
    "final_output_file = r\"C:\\Users\\Kevin Tran\\Documents\\Project Data\\feature extractions\\eeg_features.csv\"\n",
    "\n",
    "os.makedirs(output_feature_dir, exist_ok=True)  # Ensure temp folder exists\n",
    "os.makedirs(os.path.dirname(final_output_file), exist_ok=True)  # Ensure output folder exists\n",
    "\n",
    "processed_files = [os.path.join(processed_eegs_dir, f) for f in os.listdir(processed_eegs_dir) if f.endswith(\".parquet\")]\n",
    "\n",
    "# Sampling frequency (Hz) and window size (in seconds)\n",
    "fs = 200  # Sampling frequency\n",
    "window_size_seconds = 2  # Duration of each window in seconds\n",
    "window_size_samples = fs * window_size_seconds  # Convert window size to samples\n",
    "\n",
    "# Function to split data into windows\n",
    "def split_into_windows(signal, window_size):\n",
    "    num_windows = len(signal) // window_size\n",
    "    return [signal[i * window_size:(i + 1) * window_size] for i in range(num_windows)]\n",
    "\n",
    "# Function to extract time-domain features\n",
    "def extract_time_features(signal):\n",
    "    return {\n",
    "        \"mean\": np.mean(signal),\n",
    "        \"variance\": np.var(signal),\n",
    "        \"skewness\": skew(signal),\n",
    "        \"kurtosis\": kurtosis(signal),\n",
    "        \"rms\": np.sqrt(np.mean(signal**2)),\n",
    "        \"zero_crossing_rate\": np.sum(np.diff(np.sign(signal)) != 0) / len(signal),\n",
    "    }\n",
    "\n",
    "# Function to extract frequency-domain features\n",
    "def extract_frequency_features(signal, fs):\n",
    "    freqs, psd = welch(signal, fs=fs, nperseg=256)\n",
    "    return {\n",
    "        \"delta_power\": np.sum(psd[(freqs >= 0.5) & (freqs < 4)]),\n",
    "        \"theta_power\": np.sum(psd[(freqs >= 4) & (freqs < 8)]),\n",
    "        \"alpha_power\": np.sum(psd[(freqs >= 8) & (freqs < 13)]),\n",
    "        \"beta_power\": np.sum(psd[(freqs >= 13) & (freqs < 30)]),\n",
    "        \"gamma_power\": np.sum(psd[(freqs >= 30) & (freqs < 50)]),\n",
    "        \"spectral_entropy\": -np.sum(psd * np.log(psd + 1e-10)),  # Avoid log(0)\n",
    "    }\n",
    "\n",
    "# Function to extract features from a single EEG file\n",
    "def extract_features_from_file(file_path):\n",
    "    try:\n",
    "        data = pd.read_parquet(file_path)\n",
    "        all_features = []\n",
    "\n",
    "        for channel in data.columns:\n",
    "            signal = data[channel].values\n",
    "            windows = split_into_windows(signal, window_size_samples)\n",
    "\n",
    "            for i, window in enumerate(windows):\n",
    "                time_features = extract_time_features(window)\n",
    "                frequency_features = extract_frequency_features(window, fs)\n",
    "                combined_features = {\n",
    "                    \"file\": os.path.basename(file_path),\n",
    "                    \"channel\": channel,\n",
    "                    \"window\": i,\n",
    "                    **time_features,\n",
    "                    **frequency_features,\n",
    "                }\n",
    "                all_features.append(combined_features)\n",
    "\n",
    "        output_file = os.path.join(output_feature_dir, os.path.basename(file_path).replace(\".parquet\", \".csv\"))\n",
    "        pd.DataFrame(all_features).to_csv(output_file, index=False)\n",
    "        return f\"✅ Success: {os.path.basename(file_path)}\"\n",
    "    except Exception as e:\n",
    "        return f\"❌ Failed: {os.path.basename(file_path)} | Error: {e}\"\n",
    "\n",
    "# Threaded batch processing\n",
    "def batch_extract_features(file_list, num_workers=os.cpu_count()//4):\n",
    "    batch_size = 100  # Process files in batches of 100\n",
    "    for i in range(0, len(file_list), batch_size):\n",
    "        batch = file_list[i:i + batch_size]\n",
    "        print(f\"🚀 Processing Batch {i//batch_size+1}/{len(file_list)//batch_size+1} ({len(batch)} files)...\")\n",
    "        with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "            results = list(tqdm(executor.map(extract_features_from_file, batch), total=len(batch), desc=f\"Batch {i//batch_size+1}\"))\n",
    "\n",
    "        # Print summary of results for the current batch\n",
    "        success = [res for res in results if res.startswith(\"✅\")]\n",
    "        failed = [res for res in results if res.startswith(\"❌\")]\n",
    "        print(f\"✔️ Batch Summary: {len(success)} Success, {len(failed)} Failed\\n\")\n",
    "        if failed:\n",
    "            print(f\"❌ Failed Files: {'; '.join(failed)}\\n\")\n",
    "\n",
    "# Merge all temporary files into a single CSV and clean up temporary files\n",
    "def merge_temp_files_and_cleanup(output_dir, final_file):\n",
    "    temp_files = [os.path.join(output_dir, f) for f in os.listdir(output_dir) if f.endswith(\".csv\")]\n",
    "    \n",
    "    if not temp_files:\n",
    "        print(\"❌ No temporary files found to merge!\")\n",
    "        return\n",
    "\n",
    "    print(f\"🚀 Merging {len(temp_files)} temporary files into final CSV...\")\n",
    "    combined_df = pd.concat([pd.read_csv(f) for f in temp_files], ignore_index=True)\n",
    "    combined_df.to_csv(final_file, index=False)\n",
    "    print(f\"✅ Merged files into: {final_file}\")\n",
    "\n",
    "    # Clean up temporary files\n",
    "    print(\"🧹 Cleaning up temporary files...\")\n",
    "    for temp_file in temp_files:\n",
    "        try:\n",
    "            os.remove(temp_file)\n",
    "            print(f\"🗑️ Deleted: {temp_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to delete {temp_file}: {e}\")\n",
    "\n",
    "    print(\"✅ Cleanup completed!\")\n",
    "\n",
    "# Run Feature Extraction\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"Starting feature extraction for {len(processed_files)} processed EEG files...\\n\")\n",
    "    batch_extract_features(processed_files)\n",
    "    print(f\"✅ Feature extraction completed. Temporary files saved to: {output_feature_dir}\\n\")\n",
    "\n",
    "    # Merge temporary files and clean up\n",
    "    merge_temp_files_and_cleanup(output_feature_dir, final_output_file)\n",
    "    print(\"✅ Feature extraction process fully completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.signal import welch\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# CHANGE DIRECTORY TO YOUR PROCESSED EEGS!!!\n",
    "processed_eegs_dir = r\"C:\\Users\\Kevin Tran\\Documents\\Project Data\\processed_eegs\"\n",
    "\n",
    "# List all .parquet files in the specified directory\n",
    "processed_files = [os.path.join(processed_eegs_dir, f) for f in os.listdir(processed_eegs_dir) if f.endswith(\".parquet\")]\n",
    "\n",
    "# Sampling frequency (Hz) and window size (in seconds)\n",
    "fs = 200  # Sampling frequency (200 samples per second)\n",
    "window_size_seconds = 2  # Duration of each window in seconds\n",
    "window_size_samples = fs * window_size_seconds  # Convert window size to samples\n",
    "\n",
    "\n",
    "# Function to split data into fixed-size windows\n",
    "def split_into_windows(signal, window_size):\n",
    "    \"\"\"\n",
    "    Splits the input signal into fixed-size windows.\n",
    "    Args:\n",
    "        signal: The input EEG signal (1D array).\n",
    "        window_size: Number of samples per window.\n",
    "    Returns:\n",
    "        A list of equally-sized signal windows.\n",
    "    \"\"\"\n",
    "    num_windows = len(signal) // window_size  # Calculate the number of complete windows\n",
    "    windows = [signal[i * window_size:(i + 1) * window_size] for i in range(num_windows)]  # Slice the signal\n",
    "    return windows\n",
    "\n",
    "\n",
    "# Function to extract time-domain features\n",
    "def extract_time_features(signal):\n",
    "    \"\"\"\n",
    "    Extracts statistical features from the signal in the time domain.\n",
    "    Args:\n",
    "        signal: The input EEG signal (1D array).\n",
    "    Returns:\n",
    "        A dictionary containing the computed features.\n",
    "    \"\"\"\n",
    "    features = {\n",
    "        \"mean\": np.mean(signal),  # Average value of the signal\n",
    "        \"variance\": np.var(signal),  # Variability in the signal\n",
    "        \"skewness\": skew(signal),  # Asymmetry of the signal\n",
    "        \"kurtosis\": kurtosis(signal),  # \"Peakedness\" of the signal\n",
    "        \"rms\": np.sqrt(np.mean(signal**2)),  # Root Mean Square of the signal\n",
    "        \"zero_crossing_rate\": np.sum(np.diff(np.sign(signal)) != 0) / len(signal)  # Rate of zero crossings\n",
    "    }\n",
    "    return features\n",
    "\n",
    "\n",
    "# Function to extract frequency-domain features\n",
    "def extract_frequency_features(signal, fs):\n",
    "    \"\"\"\n",
    "    Extracts frequency domain features from the signal using Welch's method.\n",
    "    Args:\n",
    "        signal: The input EEG signal (1D array).\n",
    "        fs: Sampling frequency of the signal.\n",
    "    Returns:\n",
    "        A dictionary containing the computed frequency-domain features.\n",
    "    \"\"\"\n",
    "    # Compute Power Spectral Density (PSD) using Welch's method\n",
    "    freqs, psd = welch(signal, fs=fs, nperseg=256)\n",
    "    \n",
    "    # Calculate band powers and spectral entropy\n",
    "    band_powers = {\n",
    "        \"delta_power\": np.sum(psd[(freqs >= 0.5) & (freqs < 4)]),  # Power in delta band (0.5–4 Hz)\n",
    "        \"theta_power\": np.sum(psd[(freqs >= 4) & (freqs < 8)]),  # Power in theta band (4–8 Hz)\n",
    "        \"alpha_power\": np.sum(psd[(freqs >= 8) & (freqs < 13)]),  # Power in alpha band (8–13 Hz)\n",
    "        \"beta_power\": np.sum(psd[(freqs >= 13) & (freqs < 30)]),  # Power in beta band (13–30 Hz)\n",
    "        \"gamma_power\": np.sum(psd[(freqs >= 30) & (freqs < 50)]),  # Power in gamma band (30–50 Hz)\n",
    "        \"spectral_entropy\": -np.sum(psd * np.log(psd + 1e-10))  # Avoid log(0) with small offset\n",
    "    }\n",
    "    return band_powers\n",
    "\n",
    "\n",
    "# Function to extract features from a single EEG file\n",
    "def extract_features_from_file(file_path, fs, window_size_samples):\n",
    "    \"\"\"\n",
    "    Processes an EEG file to extract both time-domain and frequency-domain features.\n",
    "    Args:\n",
    "        file_path: Path to the EEG file.\n",
    "        fs: Sampling frequency of the signal.\n",
    "        window_size_samples: Number of samples in each window.\n",
    "    Returns:\n",
    "        A DataFrame containing features for all channels and windows.\n",
    "    \"\"\"\n",
    "    data = pd.read_parquet(file_path)  # Load EEG data\n",
    "    print(f\"Processing file: {file_path}\")\n",
    "\n",
    "    all_features = []  # List to store features for all windows and channels\n",
    "    \n",
    "    # Process each channel in the EEG data\n",
    "    for channel in data.columns:\n",
    "        signal = data[channel].values  # Get the signal for the current channel\n",
    "        \n",
    "        # Split the signal into windows\n",
    "        windows = split_into_windows(signal, window_size_samples)\n",
    "        \n",
    "        # Extract features from each window\n",
    "        for i, window in enumerate(windows):\n",
    "            # Extract time-domain features\n",
    "            time_features = extract_time_features(window)\n",
    "            \n",
    "            # Extract frequency-domain features\n",
    "            frequency_features = extract_frequency_features(window, fs)\n",
    "            \n",
    "            # Combine features and add metadata\n",
    "            combined_features = {\n",
    "                \"file\": os.path.basename(file_path),  # File name\n",
    "                \"channel\": channel,  # Channel name\n",
    "                \"window\": i,  # Window index\n",
    "                **time_features,  # Add time-domain features\n",
    "                **frequency_features  # Add frequency-domain features\n",
    "            }\n",
    "            \n",
    "            # Append to the list of all features\n",
    "            all_features.append(combined_features)\n",
    "\n",
    "    # Return the features as a DataFrame\n",
    "    return pd.DataFrame(all_features)\n",
    "\n",
    "\n",
    "# Process the first file to retrieve channel names\n",
    "first_file_path = processed_files[0]\n",
    "data = pd.read_parquet(first_file_path)\n",
    "channels = data.columns.tolist()  # Get the list of channel names\n",
    "\n",
    "# Process all files and combine features\n",
    "combined_features = []\n",
    "\n",
    "for file_path in processed_files:\n",
    "    print(f\"Extracting features from: {file_path}\")\n",
    "    file_features = extract_features_from_file(file_path, fs, window_size_samples)\n",
    "    combined_features.append(file_features)\n",
    "\n",
    "# Combine all features into a single DataFrame\n",
    "all_features_df = pd.concat(combined_features, ignore_index=True)\n",
    "\n",
    "# Save extracted features to a CSV file for later use\n",
    "output_feature_file = r\"C:/Users/Kevin Tran/Documents/Project Data/feature extractions/eeg_features.csv\"\n",
    "all_features_df.to_csv(output_feature_file, index=False)\n",
    "\n",
    "# Preview the extracted features\n",
    "print(\"\\nExtracted Features:\")\n",
    "print(all_features_df.head())\n",
    "print(f\"Features saved to: {output_feature_file}\")\n",
    "\n",
    "# Visualize features for the first channel\n",
    "first_channel_name = channels[0]  # Select the first channel\n",
    "first_channel_features = all_features_df[all_features_df['channel'] == first_channel_name]\n",
    "\n",
    "plt.plot(first_channel_features['window'], first_channel_features['delta_power'], label='Delta Power')\n",
    "plt.title(f'Delta Power Across Windows - {first_channel_name}')\n",
    "plt.xlabel('Window Index')\n",
    "plt.ylabel('Delta Power')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Detailed anomalies saved to: C:/Users/Kevin Tran/Documents/Project Data/feature extractions/eeg_anomalies_detailed.csv\n",
      "✅ EEG ID classifications saved to: C:/Users/Kevin Tran/Documents/Project Data/feature extractions/eeg_id_classifications.csv\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# 📌 Load the extracted EEG feature dataset\n",
    "features_file = \"C:/Users/Kevin Tran/Documents/Project Data/feature extractions/eeg_features.csv\"\n",
    "df_features = pd.read_csv(features_file)\n",
    "\n",
    "# 📌 Extract EEG ID from the filename\n",
    "df_features[\"eeg_id\"] = df_features[\"file\"].str.extract(r'(\\d+)')  # Extract numeric EEG ID\n",
    "\n",
    "# 📌 Drop Non-Numeric Columns Before Handling NaNs\n",
    "numeric_columns = df_features.select_dtypes(include=[np.number]).columns\n",
    "df_features[numeric_columns] = df_features[numeric_columns].fillna(df_features[numeric_columns].mean())  # Fill NaNs\n",
    "\n",
    "# 📌 Drop non-numeric columns for Isolation Forest\n",
    "X = df_features[numeric_columns].copy()\n",
    "\n",
    "# 📌 Train the Isolation Forest Model\n",
    "iso_forest = IsolationForest(n_estimators=200, contamination=0.01, random_state=42)\n",
    "df_features[\"anomaly\"] = iso_forest.fit_predict(X)  # -1 = Seizure-Like, 1 = Non-Seizure\n",
    "\n",
    "# 📌 Save Detailed Anomalies for Each Channel\n",
    "detailed_anomalies_file = \"C:/Users/Kevin Tran/Documents/Project Data/feature extractions/eeg_anomalies_detailed.csv\"\n",
    "df_features.to_csv(detailed_anomalies_file, index=False)\n",
    "print(f\"✅ Detailed anomalies saved to: {detailed_anomalies_file}\")\n",
    "\n",
    "# 📌 Aggregate Anomalies by EEG ID\n",
    "# If any channel in an EEG ID is classified as an anomaly (-1), the entire EEG ID is classified as Seizure\n",
    "eeg_classifications = df_features.groupby(\"eeg_id\")[\"anomaly\"].apply(lambda x: -1 if (x == -1).any() else 1).reset_index()\n",
    "eeg_classifications[\"classification\"] = eeg_classifications[\"anomaly\"].map({-1: \"Seizure\", 1: \"Non-Seizure\"})\n",
    "\n",
    "# 📌 Save Results with EEG IDs\n",
    "eeg_classifications_file = \"C:/Users/Kevin Tran/Documents/Project Data/feature extractions/eeg_id_classifications.csv\"\n",
    "eeg_classifications.to_csv(eeg_classifications_file, index=False)\n",
    "print(f\"✅ EEG ID classifications saved to: {eeg_classifications_file}\")\n",
    "\n",
    "# 📊 Plot Distribution of EEG IDs (Seizure vs Non-Seizure)\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(eeg_classifications[\"classification\"], bins=2, edgecolor=\"black\", alpha=0.7)\n",
    "plt.xlabel(\"EEG Classification\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribution of Seizure vs Non-Seizure EEG IDs\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processed 4396380 rows. Total Chunks: 34\n",
      "✅ Processed 1212920 rows. Total Chunks: 35\n",
      "✅ Processed 7854820 rows. Total Chunks: 36\n",
      "✅ Processed 587480 rows. Total Chunks: 37\n",
      "✅ Processed 515960 rows. Total Chunks: 38\n",
      "✅ Processed 435260 rows. Total Chunks: 39\n",
      "✅ Processed 568900 rows. Total Chunks: 40\n",
      "✅ Processed 1004940 rows. Total Chunks: 41\n",
      "✅ Processed 1033020 rows. Total Chunks: 42\n",
      "✅ Processed 412020 rows. Total Chunks: 43\n",
      "✅ Processed 498520 rows. Total Chunks: 44\n",
      "✅ Processed 3413740 rows. Total Chunks: 45\n",
      "✅ Processed 1244500 rows. Total Chunks: 46\n",
      "✅ Processed 297520 rows. Total Chunks: 47\n",
      "✅ Processed 9634280 rows. Total Chunks: 48\n",
      "✅ Processed 6363100 rows. Total Chunks: 49\n",
      "✅ Processed 532900 rows. Total Chunks: 50\n",
      "✅ Processed 530520 rows. Total Chunks: 51\n",
      "✅ Processed 369540 rows. Total Chunks: 52\n",
      "✅ Processed 318280 rows. Total Chunks: 53\n",
      "✅ Processed 5396140 rows. Total Chunks: 54\n",
      "✅ Processed 388300 rows. Total Chunks: 55\n",
      "✅ Processed 526440 rows. Total Chunks: 56\n",
      "✅ Processed 356860 rows. Total Chunks: 57\n",
      "✅ Processed 585480 rows. Total Chunks: 58\n",
      "✅ Processed 436080 rows. Total Chunks: 59\n",
      "✅ Processed 267620 rows. Total Chunks: 60\n",
      "✅ Processed 461260 rows. Total Chunks: 61\n",
      "✅ Processed 238500 rows. Total Chunks: 62\n",
      "✅ Processed 702960 rows. Total Chunks: 63\n",
      "✅ Processed 477960 rows. Total Chunks: 64\n",
      "✅ Processed 630360 rows. Total Chunks: 65\n",
      "✅ Processed 9771840 rows. Total Chunks: 66\n",
      "✅ Processed 2020420 rows. Total Chunks: 67\n",
      "✅ Processed 675060 rows. Total Chunks: 68\n",
      "✅ Processed 712280 rows. Total Chunks: 69\n",
      "✅ Processed 606020 rows. Total Chunks: 70\n",
      "✅ Processed 275740 rows. Total Chunks: 71\n",
      "✅ Processed 1345600 rows. Total Chunks: 72\n",
      "✅ Processed 561120 rows. Total Chunks: 73\n",
      "✅ Processed 396060 rows. Total Chunks: 74\n",
      "✅ Processed 418220 rows. Total Chunks: 75\n",
      "✅ Processed 500560 rows. Total Chunks: 76\n",
      "✅ Processed 534600 rows. Total Chunks: 77\n",
      "✅ Processed 737780 rows. Total Chunks: 78\n",
      "✅ Processed 676520 rows. Total Chunks: 79\n",
      "✅ Processed 374560 rows. Total Chunks: 80\n",
      "✅ Processed 641960 rows. Total Chunks: 81\n",
      "✅ Processed 3179500 rows. Total Chunks: 82\n",
      "✅ Processed 1072060 rows. Total Chunks: 83\n",
      "✅ Processed 308500 rows. Total Chunks: 84\n",
      "✅ Processed 321620 rows. Total Chunks: 85\n",
      "✅ Processed 536020 rows. Total Chunks: 86\n",
      "✅ Processed 473400 rows. Total Chunks: 87\n",
      "✅ Processed 605660 rows. Total Chunks: 88\n",
      "✅ Processed 313500 rows. Total Chunks: 89\n",
      "✅ Processed 589240 rows. Total Chunks: 90\n",
      "✅ Processed 721320 rows. Total Chunks: 91\n",
      "✅ Processed 463600 rows. Total Chunks: 92\n",
      "✅ Processed 16370100 rows. Total Chunks: 93\n",
      "✅ Processed 9512720 rows. Total Chunks: 94\n",
      "✅ Processed 911080 rows. Total Chunks: 95\n",
      "✅ Processed 463320 rows. Total Chunks: 96\n",
      "✅ Processed 647280 rows. Total Chunks: 97\n",
      "✅ Processed 302000 rows. Total Chunks: 98\n",
      "✅ Processed 506800 rows. Total Chunks: 99\n",
      "✅ Processed 621240 rows. Total Chunks: 100\n",
      "✅ Processed 721620 rows. Total Chunks: 101\n",
      "✅ Processed 313520 rows. Total Chunks: 102\n",
      "✅ Processed 2211420 rows. Total Chunks: 103\n",
      "✅ Processed 263620 rows. Total Chunks: 104\n",
      "✅ Processed 895920 rows. Total Chunks: 105\n",
      "✅ Processed 19902120 rows. Total Chunks: 106\n",
      "✅ Processed 3067320 rows. Total Chunks: 107\n",
      "✅ Processed 531080 rows. Total Chunks: 108\n",
      "✅ Processed 340400 rows. Total Chunks: 109\n",
      "✅ Processed 400700 rows. Total Chunks: 110\n",
      "✅ Processed 617560 rows. Total Chunks: 111\n",
      "✅ Processed 451520 rows. Total Chunks: 112\n",
      "✅ Processed 525460 rows. Total Chunks: 113\n",
      "✅ Processed 556120 rows. Total Chunks: 114\n",
      "✅ Processed 302900 rows. Total Chunks: 115\n",
      "✅ Processed 290700 rows. Total Chunks: 116\n",
      "✅ Processed 388880 rows. Total Chunks: 117\n",
      "✅ Processed 388560 rows. Total Chunks: 118\n",
      "✅ Processed 379540 rows. Total Chunks: 119\n",
      "✅ Processed 525360 rows. Total Chunks: 120\n",
      "✅ Processed 581680 rows. Total Chunks: 121\n",
      "✅ Processed 381940 rows. Total Chunks: 122\n",
      "✅ Processed 515380 rows. Total Chunks: 123\n",
      "✅ Processed 384960 rows. Total Chunks: 124\n",
      "✅ Processed 793180 rows. Total Chunks: 125\n",
      "✅ Processed 540960 rows. Total Chunks: 126\n",
      "✅ Processed 530320 rows. Total Chunks: 127\n",
      "✅ Processed 499000 rows. Total Chunks: 128\n",
      "✅ Processed 438420 rows. Total Chunks: 129\n",
      "✅ Processed 570120 rows. Total Chunks: 130\n",
      "✅ Processed 317360 rows. Total Chunks: 131\n",
      "✅ Processed 422900 rows. Total Chunks: 132\n",
      "✅ Processed 325680 rows. Total Chunks: 133\n",
      "✅ Processed 330180 rows. Total Chunks: 134\n",
      "✅ Processed 404720 rows. Total Chunks: 135\n",
      "✅ Processed 384080 rows. Total Chunks: 136\n",
      "✅ Processed 13115200 rows. Total Chunks: 137\n",
      "✅ Processed 435560 rows. Total Chunks: 138\n",
      "✅ Processed 1332060 rows. Total Chunks: 139\n",
      "✅ Processed 1120360 rows. Total Chunks: 140\n",
      "✅ Processed 351200 rows. Total Chunks: 141\n",
      "✅ Processed 237160 rows. Total Chunks: 142\n",
      "✅ Processed 362360 rows. Total Chunks: 143\n",
      "✅ Processed 997400 rows. Total Chunks: 144\n",
      "✅ Processed 266480 rows. Total Chunks: 145\n",
      "✅ Processed 501480 rows. Total Chunks: 146\n",
      "✅ Processed 358720 rows. Total Chunks: 147\n",
      "✅ Processed 559140 rows. Total Chunks: 148\n",
      "✅ Processed 323720 rows. Total Chunks: 149\n",
      "✅ Processed 477440 rows. Total Chunks: 150\n",
      "✅ Processed 285820 rows. Total Chunks: 151\n",
      "✅ Processed 537480 rows. Total Chunks: 152\n",
      "✅ Processed 477820 rows. Total Chunks: 153\n",
      "✅ Processed 422020 rows. Total Chunks: 154\n",
      "✅ Processed 2450200 rows. Total Chunks: 155\n",
      "✅ Processed 1150980 rows. Total Chunks: 156\n",
      "✅ Processed 368100 rows. Total Chunks: 157\n",
      "✅ Processed 367420 rows. Total Chunks: 158\n",
      "✅ Processed 563800 rows. Total Chunks: 159\n",
      "✅ Processed 396380 rows. Total Chunks: 160\n",
      "✅ Processed 992040 rows. Total Chunks: 161\n",
      "✅ Processed 433800 rows. Total Chunks: 162\n",
      "✅ Processed 439020 rows. Total Chunks: 163\n",
      "✅ Processed 3048500 rows. Total Chunks: 164\n",
      "✅ Processed 514800 rows. Total Chunks: 165\n",
      "✅ Processed 354300 rows. Total Chunks: 166\n",
      "✅ Processed 337760 rows. Total Chunks: 167\n",
      "✅ Processed 1189900 rows. Total Chunks: 168\n",
      "✅ Processed 744060 rows. Total Chunks: 169\n",
      "✅ Processed 1385080 rows. Total Chunks: 170\n",
      "✅ Processed 568960 rows. Total Chunks: 171\n",
      "✅ Processed 339320 rows. Total Chunks: 172\n",
      "✅ Processed 411180 rows. Total Chunks: 173\n",
      "✅ Processed 741300 rows. Total Chunks: 174\n",
      "✅ Processed 1451500 rows. Total Chunks: 175\n",
      "✅ Processed 468560 rows. Total Chunks: 176\n",
      "✅ Processed 307840 rows. Total Chunks: 177\n",
      "✅ Processed 338480 rows. Total Chunks: 178\n",
      "✅ Processed 500420 rows. Total Chunks: 179\n",
      "✅ Processed 347400 rows. Total Chunks: 180\n",
      "✅ Processed 353200 rows. Total Chunks: 181\n",
      "✅ Processed 384120 rows. Total Chunks: 182\n",
      "✅ Processed 358720 rows. Total Chunks: 183\n",
      "✅ Processed 3123900 rows. Total Chunks: 184\n",
      "✅ Processed 527100 rows. Total Chunks: 185\n",
      "✅ Processed 365440 rows. Total Chunks: 186\n",
      "✅ Processed 953380 rows. Total Chunks: 187\n",
      "✅ Processed 960840 rows. Total Chunks: 188\n",
      "✅ Processed 450760 rows. Total Chunks: 189\n",
      "✅ Processed 431660 rows. Total Chunks: 190\n",
      "✅ Processed 566860 rows. Total Chunks: 191\n",
      "✅ Processed 390480 rows. Total Chunks: 192\n",
      "✅ Processed 432820 rows. Total Chunks: 193\n",
      "✅ Processed 1561220 rows. Total Chunks: 194\n",
      "✅ Processed 798520 rows. Total Chunks: 195\n",
      "✅ Processed 1863160 rows. Total Chunks: 196\n",
      "✅ Processed 508940 rows. Total Chunks: 197\n",
      "✅ Processed 401620 rows. Total Chunks: 198\n",
      "✅ Processed 708420 rows. Total Chunks: 199\n",
      "✅ Processed 950140 rows. Total Chunks: 200\n",
      "✅ Processed 909480 rows. Total Chunks: 201\n",
      "✅ Processed 426740 rows. Total Chunks: 202\n",
      "✅ Processed 471220 rows. Total Chunks: 203\n",
      "✅ Processed 1230460 rows. Total Chunks: 204\n",
      "✅ Processed 470540 rows. Total Chunks: 205\n",
      "✅ Processed 366960 rows. Total Chunks: 206\n",
      "✅ Processed 389240 rows. Total Chunks: 207\n",
      "✅ Processed 301440 rows. Total Chunks: 208\n",
      "✅ Processed 316880 rows. Total Chunks: 209\n",
      "✅ Processed 382100 rows. Total Chunks: 210\n",
      "✅ Processed 292540 rows. Total Chunks: 211\n",
      "✅ Processed 411440 rows. Total Chunks: 212\n",
      "✅ Processed 423280 rows. Total Chunks: 213\n",
      "✅ Processed 192920 rows. Total Chunks: 214\n",
      "✅ Processed 665580 rows. Total Chunks: 215\n",
      "✅ Processed 330280 rows. Total Chunks: 216\n",
      "✅ Processed 326960 rows. Total Chunks: 217\n",
      "✅ Processed 1068980 rows. Total Chunks: 218\n",
      "✅ Processed 444620 rows. Total Chunks: 219\n",
      "✅ Processed 729840 rows. Total Chunks: 220\n",
      "✅ Processed 767400 rows. Total Chunks: 221\n",
      "✅ Processed 1323940 rows. Total Chunks: 222\n",
      "✅ Processed 679720 rows. Total Chunks: 223\n",
      "✅ Processed 1078600 rows. Total Chunks: 224\n",
      "✅ Processed 478400 rows. Total Chunks: 225\n",
      "✅ Processed 570340 rows. Total Chunks: 226\n",
      "✅ Processed 2486320 rows. Total Chunks: 227\n",
      "✅ Processed 1505480 rows. Total Chunks: 228\n",
      "✅ Processed 493060 rows. Total Chunks: 229\n",
      "✅ Processed 526540 rows. Total Chunks: 230\n",
      "✅ Processed 307040 rows. Total Chunks: 231\n",
      "✅ Processed 1905640 rows. Total Chunks: 232\n",
      "✅ Processed 265300 rows. Total Chunks: 233\n",
      "✅ Processed 2735240 rows. Total Chunks: 234\n",
      "✅ Processed 682840 rows. Total Chunks: 235\n",
      "✅ Processed 311500 rows. Total Chunks: 236\n",
      "✅ Processed 425360 rows. Total Chunks: 237\n",
      "✅ Processed 400420 rows. Total Chunks: 238\n",
      "✅ Processed 365720 rows. Total Chunks: 239\n",
      "✅ Processed 670420 rows. Total Chunks: 240\n",
      "✅ Processed 2323600 rows. Total Chunks: 241\n",
      "✅ Processed 745480 rows. Total Chunks: 242\n",
      "✅ Processed 445280 rows. Total Chunks: 243\n",
      "✅ Processed 740620 rows. Total Chunks: 244\n",
      "✅ Processed 7847620 rows. Total Chunks: 245\n",
      "✅ Processed 854180 rows. Total Chunks: 246\n",
      "✅ Processed 357760 rows. Total Chunks: 247\n",
      "✅ Processed 1637440 rows. Total Chunks: 248\n",
      "✅ Processed 2009780 rows. Total Chunks: 249\n",
      "✅ Processed 464720 rows. Total Chunks: 250\n",
      "✅ Processed 298700 rows. Total Chunks: 251\n",
      "✅ Processed 316160 rows. Total Chunks: 252\n",
      "✅ Processed 1775420 rows. Total Chunks: 253\n",
      "✅ Processed 601060 rows. Total Chunks: 254\n",
      "✅ Processed 721740 rows. Total Chunks: 255\n",
      "✅ Processed 489660 rows. Total Chunks: 256\n",
      "✅ Processed 311500 rows. Total Chunks: 257\n",
      "✅ Processed 2307260 rows. Total Chunks: 258\n",
      "✅ Processed 1243480 rows. Total Chunks: 259\n",
      "✅ Processed 2242760 rows. Total Chunks: 260\n",
      "✅ Processed 484740 rows. Total Chunks: 261\n",
      "✅ Processed 496700 rows. Total Chunks: 262\n",
      "✅ Processed 653420 rows. Total Chunks: 263\n",
      "✅ Processed 2075500 rows. Total Chunks: 264\n",
      "✅ Processed 895980 rows. Total Chunks: 265\n",
      "✅ Processed 399940 rows. Total Chunks: 266\n",
      "✅ Processed 434940 rows. Total Chunks: 267\n",
      "✅ Processed 481020 rows. Total Chunks: 268\n",
      "✅ Processed 4080340 rows. Total Chunks: 269\n",
      "✅ Processed 1151200 rows. Total Chunks: 270\n",
      "✅ Processed 846300 rows. Total Chunks: 271\n",
      "✅ Processed 273600 rows. Total Chunks: 272\n",
      "✅ Processed 284540 rows. Total Chunks: 273\n",
      "✅ Processed 426920 rows. Total Chunks: 274\n",
      "✅ Processed 266320 rows. Total Chunks: 275\n",
      "✅ Processed 714280 rows. Total Chunks: 276\n",
      "✅ Processed 333720 rows. Total Chunks: 277\n",
      "✅ Processed 2165640 rows. Total Chunks: 278\n",
      "✅ Processed 572860 rows. Total Chunks: 279\n",
      "✅ Processed 797740 rows. Total Chunks: 280\n",
      "✅ Processed 438160 rows. Total Chunks: 281\n",
      "✅ All data successfully processed and saved to: C:\\Users\\Kevin Tran\\Documents\\Project Data\\feature extractions\\eeg_features_processed.csv\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 38\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ All data successfully processed and saved to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprocessed_output_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# 📌 Load the Processed File (Much Smaller Than Before)\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_output_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# 📌 Drop Unnecessary Columns\u001b[39;00m\n\u001b[0;32m     41\u001b[0m df\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchannel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwindow\u001b[39m\u001b[38;5;124m\"\u001b[39m], inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Kevin Tran\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Kevin Tran\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:617\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Kevin Tran\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1748\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1741\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1743\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m     (\n\u001b[0;32m   1745\u001b[0m         index,\n\u001b[0;32m   1746\u001b[0m         columns,\n\u001b[0;32m   1747\u001b[0m         col_dict,\n\u001b[1;32m-> 1748\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1749\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1750\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1751\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1752\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\Kevin Tran\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mparsers.pyx:843\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:904\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:879\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:890\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2058\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "#machine learning test 1\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "# 📌 Define File Paths\n",
    "train_labels_path = r\"C:\\Users\\Kevin Tran\\Documents\\GitHub ED1\\hms-harmful-brain-activity-classificationtrain_eegs\\train.csv\"\n",
    "features_file_path = r\"C:\\Users\\Kevin Tran\\Documents\\Project Data\\feature extractions\\eeg_features.csv\"\n",
    "processed_output_path = r\"C:\\Users\\Kevin Tran\\Documents\\Project Data\\feature extractions\\eeg_features_processed.csv\"\n",
    "model_output_path = r\"C:\\Users\\Kevin Tran\\Documents\\Project Data\\seizure_model.pkl\"\n",
    "\n",
    "# 📌 Load Train Labels (Seizure vs. Non-Seizure)\n",
    "df_labels = pd.read_csv(train_labels_path, dtype={'eeg_id': 'str'})  # Keep EEG ID as string to save memory\n",
    "df_labels.rename(columns={\"eeg_id\": \"file\"}, inplace=True)\n",
    "\n",
    "# 📌 Create a File to Store Processed Data (Instead of Keeping in RAM)\n",
    "if os.path.exists(processed_output_path):\n",
    "    os.remove(processed_output_path)  # Remove existing file to prevent duplicates\n",
    "\n",
    "# 📌 Process EEG Features in Chunks & Write Directly to File\n",
    "chunk_size = 50000  # Process 50,000 rows at a time\n",
    "print(\"🚀 Processing EEG feature chunks...\")\n",
    "\n",
    "for i, chunk in enumerate(pd.read_csv(features_file_path, dtype={'file': 'str'}, chunksize=chunk_size)):\n",
    "    chunk[\"file\"] = chunk[\"file\"].str.replace(\".parquet\", \"\", regex=False)  # Ensure filename consistency\n",
    "    merged_chunk = chunk.merge(df_labels, on=\"file\", how=\"inner\")  # Merge with train.csv labels\n",
    "\n",
    "    # Append to output CSV to avoid keeping everything in RAM\n",
    "    merged_chunk.to_csv(processed_output_path, mode='a', index=False, header=(i == 0))  # Write header only once\n",
    "    print(f\"✅ Processed {len(merged_chunk)} rows. Total Chunks: {i+1}\")\n",
    "\n",
    "print(f\"✅ All data successfully processed and saved to: {processed_output_path}\")\n",
    "\n",
    "# 📌 Load the Processed File (Much Smaller Than Before)\n",
    "df = pd.read_csv(processed_output_path)\n",
    "\n",
    "# 📌 Drop Unnecessary Columns\n",
    "df.drop(columns=[\"file\", \"channel\", \"window\"], inplace=True, errors=\"ignore\")\n",
    "\n",
    "# 📌 Define Features and Target Variable\n",
    "X = df.drop(columns=[\"expert_consensus\"])  # Features (EEG extracted values)\n",
    "y = df[\"expert_consensus\"].map({\"seizure\": 1, \"non_seizure\": 0})  # Convert labels to binary (1=Seizure, 0=Non-Seizure)\n",
    "\n",
    "# 📌 Reduce Memory Usage by Converting to Float32\n",
    "X = X.astype(np.float32)\n",
    "\n",
    "# 📌 Train-Test Split (80% Training, 20% Testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# 📌 Train a Machine Learning Model (Random Forest)\n",
    "model = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 📌 Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 📌 Evaluate Model Performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"✅ Model Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\n📊 Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# 📌 Save Model\n",
    "joblib.dump(model, model_output_path)\n",
    "print(f\"✅ Model saved at: {model_output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Loading EEG Feature Data...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# 📌 Load EEG Feature Extraction Data\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🚀 Loading EEG Feature Data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfile\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpython\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon_bad_lines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mskip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# 📌 Drop Unnecessary Columns\u001b[39;00m\n\u001b[0;32m     19\u001b[0m df\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchannel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwindow\u001b[39m\u001b[38;5;124m\"\u001b[39m], inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Kevin Tran\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Kevin Tran\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:617\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Kevin Tran\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1748\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1741\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1743\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m     (\n\u001b[0;32m   1745\u001b[0m         index,\n\u001b[0;32m   1746\u001b[0m         columns,\n\u001b[0;32m   1747\u001b[0m         col_dict,\n\u001b[1;32m-> 1748\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1749\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1750\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1751\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1752\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\Kevin Tran\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\python_parser.py:250\u001b[0m, in \u001b[0;36mPythonParser.read\u001b[1;34m(self, rows)\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread\u001b[39m(\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;28mself\u001b[39m, rows: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    246\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[\n\u001b[0;32m    247\u001b[0m     Index \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, Sequence[Hashable] \u001b[38;5;241m|\u001b[39m MultiIndex, Mapping[Hashable, ArrayLike]\n\u001b[0;32m    248\u001b[0m ]:\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 250\u001b[0m         content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_lines\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_first_chunk:\n",
      "File \u001b[1;32mc:\\Users\\Kevin Tran\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\python_parser.py:1135\u001b[0m, in \u001b[0;36mPythonParser._get_lines\u001b[1;34m(self, rows)\u001b[0m\n\u001b[0;32m   1132\u001b[0m rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   1134\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1135\u001b[0m     new_row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_iter_line\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow_num\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrows\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1136\u001b[0m     rows \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1138\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m new_row \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Kevin Tran\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\python_parser.py:800\u001b[0m, in \u001b[0;36mPythonParser._next_iter_line\u001b[1;34m(self, row_num)\u001b[0m\n\u001b[0;32m    797\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    798\u001b[0m     \u001b[38;5;66;03m# assert for mypy, data is Iterator[str] or None, would error in next\u001b[39;00m\n\u001b[0;32m    799\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 800\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    801\u001b[0m     \u001b[38;5;66;03m# for mypy\u001b[39;00m\n\u001b[0;32m    802\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(line, \u001b[38;5;28mlist\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "# 📌 Define Correct File Paths\n",
    "features_file_path = r\"C:\\Users\\Kevin Tran\\Documents\\Project Data\\feature extractions\\eeg_features.csv\"\n",
    "train_labels_path = r\"C:\\Users\\Kevin Tran\\Documents\\GitHub ED1\\hms-harmful-brain-activity-classificationtrain_eegs\\train.csv\"\n",
    "model_output_path = r\"C:\\Users\\Kevin Tran\\Documents\\Project Data\\seizure_model.pkl\"\n",
    "\n",
    "# 📌 Load EEG Feature Extraction Data\n",
    "print(\"🚀 Loading EEG Feature Data...\")\n",
    "df = pd.read_csv(features_file_path, dtype={'file': 'str'}, engine=\"python\", on_bad_lines=\"skip\")\n",
    "\n",
    "# 📌 Drop Unnecessary Columns\n",
    "df.drop(columns=[\"channel\", \"window\"], inplace=True, errors=\"ignore\")\n",
    "\n",
    "# ✅ Remove `.parquet` from EEG File Names & Ensure Consistency\n",
    "df[\"file\"] = df[\"file\"].str.replace(\".parquet\", \"\", regex=False).str.strip()\n",
    "\n",
    "# 📌 Aggregate Data: Group by EEG ID & Compute Mean of Features\n",
    "print(\"🔄 Grouping EEG Data by ID...\")\n",
    "df_grouped = df.groupby(\"file\", as_index=False).mean()\n",
    "\n",
    "# 📌 Load Train Labels (Seizure vs. Non-Seizure)\n",
    "df_labels = pd.read_csv(train_labels_path, dtype={'eeg_id': 'str'})  # Ensure EEG IDs are strings\n",
    "df_labels.rename(columns={\"eeg_id\": \"file\"}, inplace=True)\n",
    "\n",
    "# ✅ Standardize & Strip Spaces from EEG IDs\n",
    "df_grouped[\"file\"] = df_grouped[\"file\"].astype(str).str.strip()\n",
    "df_labels[\"file\"] = df_labels[\"file\"].astype(str).str.strip()\n",
    "\n",
    "# ✅ DEBUG: Print Number of Matching EEG IDs Before Merging\n",
    "matching_ids = set(df_grouped[\"file\"]).intersection(set(df_labels[\"file\"]))\n",
    "print(f\"🔍 {len(matching_ids)} EEG IDs match between feature extraction & train.csv.\")\n",
    "\n",
    "# ✅ Merge EEG Features with Labels (Keeping Only Matching EEGs)\n",
    "df_grouped = df_grouped.merge(df_labels, on=\"file\", how=\"inner\")\n",
    "\n",
    "# ✅ FIX: Filter Only Valid Labels (\"seizure\" or \"non_seizure\")\n",
    "df_grouped = df_grouped[df_grouped[\"expert_consensus\"].isin([\"seizure\", \"non_seizure\"])]\n",
    "\n",
    "# 📌 Define Features & Target\n",
    "X = df_grouped.drop(columns=[\"file\", \"expert_consensus\"])\n",
    "y = df_grouped[\"expert_consensus\"].map({\"seizure\": 1, \"non_seizure\": 0})  # Convert labels to binary\n",
    "\n",
    "# ✅ DEBUG: Ensure No NaN in `y` (Final Check)\n",
    "if y.isna().sum() > 0:\n",
    "    print(\"🚨 ERROR: NaN values still found in `y`. Investigating...\")\n",
    "    print(df_grouped[df_grouped[\"expert_consensus\"].isna()].head(20))  # Print problematic rows\n",
    "    raise ValueError(f\"🚨 Found {y.isna().sum()} NaN values in `y`!\")\n",
    "\n",
    "# 📌 Reduce Memory Usage\n",
    "X = X.astype(np.float16)  # Convert to `float16` to save memory\n",
    "\n",
    "# 📌 Train-Test Split (80% Train, 20% Test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# ✅ Notify When Test Data is Being Processed\n",
    "print(f\"🔔 {len(X_test)} EEG IDs selected for testing... Model Training Begins! 🚀\")\n",
    "\n",
    "# 📌 Train a Machine Learning Model (Random Forest - Optimized for Memory)\n",
    "model = RandomForestClassifier(n_estimators=100, max_depth=15, n_jobs=-1, random_state=42)  # Fewer trees, max depth to save RAM\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 📌 Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 📌 Evaluate Model Performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"✅ Model Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\n📊 Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# 📌 Save Model\n",
    "joblib.dump(model, model_output_path)\n",
    "print(f\"✅ Model saved at: {model_output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 file channel  window      mean  variance  skewness  kurtosis  \\\n",
      "0  1000913311.parquet     Fp1       0  0.171571  2.755021 -0.352579 -0.886731   \n",
      "1  1000913311.parquet     Fp1       1  0.124090  1.608011 -0.210350 -0.174512   \n",
      "2  1000913311.parquet     Fp1       2 -0.077944  0.829873  0.301777 -0.110549   \n",
      "3  1000913311.parquet     Fp1       3 -0.014668  0.654192 -0.581156  0.979690   \n",
      "4  1000913311.parquet     Fp1       4  0.038659  0.498066  0.602220  1.722742   \n",
      "\n",
      "        rms  zero_crossing_rate  delta_power  theta_power  alpha_power  \\\n",
      "0  1.668669              0.0275     3.634739     0.076030     0.020288   \n",
      "1  1.274131              0.0450     1.865378     0.034776     0.028235   \n",
      "2  0.914302              0.0850     0.750413     0.139047     0.033155   \n",
      "3  0.808955              0.0775     0.536515     0.081503     0.075334   \n",
      "4  0.706796              0.1125     0.345207     0.040682     0.032545   \n",
      "\n",
      "   beta_power  gamma_power  spectral_entropy  \n",
      "0    0.196447     0.038232          0.301411  \n",
      "1    0.171878     0.043572          2.143733  \n",
      "2    0.160865     0.044616          3.006340  \n",
      "3    0.184276     0.032681          2.807163  \n",
      "4    0.233470     0.079339          2.713982  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = r\"C:\\Users\\Kevin Tran\\Documents\\Project Data\\feature extractions\\eeg_features.csv\"\n",
    "\n",
    "# Read first 20 rows without loading entire file\n",
    "df = pd.read_csv(file_path, nrows=20)\n",
    "print(df.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

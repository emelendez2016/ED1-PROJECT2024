{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Working\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# 📂 Define File Paths\n",
    "eeg_features_path = r\"C:\\Users\\Kevin Tran\\Documents\\Project Data\\feature extractions\\eeg_features.csv\"\n",
    "train_path = r\"C:\\Users\\Kevin Tran\\Documents\\GitHub ED1\\hms-harmful-brain-activity-classificationtrain_eegs\\train.csv\"\n",
    "merged_eeg_path = r\"C:\\Users\\Kevin Tran\\Documents\\Project Data\\merged_eeg_data.csv\"\n",
    "\n",
    "# ✅ Load train.csv\n",
    "train_df = pd.read_csv(train_path)\n",
    "\n",
    "# 🔄 Convert `expert_consensus` into binary labels (Seizure = 1, Non-Seizure = 0)\n",
    "train_df[\"label\"] = train_df[\"expert_consensus\"].apply(lambda x: 1 if x == \"Seizure\" else 0)\n",
    "\n",
    "# 🎯 Keep only necessary columns\n",
    "train_df = train_df[[\"eeg_id\", \"label\"]]\n",
    "\n",
    "# 🔄 Ensure `eeg_id` is a string for accurate merging\n",
    "train_df[\"eeg_id\"] = train_df[\"eeg_id\"].astype(str)\n",
    "\n",
    "# 🛠 Define chunk size for memory efficiency\n",
    "chunk_size = 100000\n",
    "\n",
    "# 🚀 Initialize CSV file (overwrite if exists)\n",
    "with open(merged_eeg_path, \"w\", newline=\"\") as f:\n",
    "    pass  # Just creating/clearing the file\n",
    "\n",
    "# 🔄 Process EEG features in chunks and save incrementally\n",
    "for i, chunk in enumerate(pd.read_csv(eeg_features_path, chunksize=chunk_size, dtype={\"file\": str})):\n",
    "    print(f\"🔄 Processing chunk {i + 1}\")\n",
    "\n",
    "    # 🏷 Extract `eeg_id` from 'file' column (remove `.parquet` extension)\n",
    "    chunk[\"eeg_id\"] = chunk[\"file\"].str.replace(\".parquet\", \"\", regex=False)\n",
    "\n",
    "    # 🔄 Merge chunk with `train.csv` labels\n",
    "    merged_chunk = chunk.merge(train_df, on=\"eeg_id\", how=\"left\")\n",
    "\n",
    "    # ✅ Append to CSV (without keeping everything in memory)\n",
    "    merged_chunk.to_csv(merged_eeg_path, mode=\"a\", index=False, header=(i == 0))  # Write header only for first chunk\n",
    "\n",
    "    print(f\"✅ Chunk {i + 1} saved.\")\n",
    "\n",
    "print(f\"🎉 Merging completed. Data saved at: {merged_eeg_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Loading EEG data in chunks...\n",
      "🔄 Processing chunk 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kevin Tran\\AppData\\Local\\Temp\\ipykernel_23280\\2659928150.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_chunk.fillna(X_chunk.median(), inplace=True)\n",
      "C:\\Users\\Kevin Tran\\AppData\\Local\\Temp\\ipykernel_23280\\2659928150.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_chunk.fillna(X_chunk.median(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Processing chunk 2...\n",
      "🔄 Processing chunk 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kevin Tran\\AppData\\Local\\Temp\\ipykernel_23280\\2659928150.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_chunk.fillna(X_chunk.median(), inplace=True)\n",
      "C:\\Users\\Kevin Tran\\AppData\\Local\\Temp\\ipykernel_23280\\2659928150.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_chunk.fillna(X_chunk.median(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Processing chunk 4...\n",
      "🔄 Processing chunk 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kevin Tran\\AppData\\Local\\Temp\\ipykernel_23280\\2659928150.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_chunk.fillna(X_chunk.median(), inplace=True)\n",
      "C:\\Users\\Kevin Tran\\AppData\\Local\\Temp\\ipykernel_23280\\2659928150.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_chunk.fillna(X_chunk.median(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Processing chunk 6...\n",
      "🔄 Processing chunk 7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kevin Tran\\AppData\\Local\\Temp\\ipykernel_23280\\2659928150.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_chunk.fillna(X_chunk.median(), inplace=True)\n",
      "C:\\Users\\Kevin Tran\\AppData\\Local\\Temp\\ipykernel_23280\\2659928150.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_chunk.fillna(X_chunk.median(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Processing chunk 8...\n",
      "🔄 Processing chunk 9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kevin Tran\\AppData\\Local\\Temp\\ipykernel_23280\\2659928150.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_chunk.fillna(X_chunk.median(), inplace=True)\n",
      "C:\\Users\\Kevin Tran\\AppData\\Local\\Temp\\ipykernel_23280\\2659928150.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_chunk.fillna(X_chunk.median(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Processing chunk 10...\n",
      "🔄 Processing chunk 11...\n",
      "⏸ Stopping early to avoid memory issues (Adjust if needed)\n",
      "✅ Final Data Shape: X=(550000, 19), y=(550000,)\n",
      "🔄 Splitting data into training and testing sets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kevin Tran\\AppData\\Local\\Temp\\ipykernel_23280\\2659928150.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_chunk.fillna(X_chunk.median(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Training Random Forest model...\n",
      "✅ Model saved to: C:\\Users\\Kevin Tran\\Documents\\Project Data\\For Machine Learning\\random_forest_model.pkl\n",
      "✅ Saved expected feature columns: C:\\Users\\Kevin Tran\\Documents\\Project Data\\For Machine Learning\\feature_columns.pkl\n",
      "✅ Model training completed. Evaluating performance...\n",
      "🎯 Accuracy: 0.9825\n",
      "\n",
      "🔍 Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      1.00      0.99     79653\n",
      "         1.0       0.99      0.95      0.97     30347\n",
      "\n",
      "    accuracy                           0.98    110000\n",
      "   macro avg       0.99      0.97      0.98    110000\n",
      "weighted avg       0.98      0.98      0.98    110000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#WORKING it makes feature columns.pkl as well\n",
    "# Machine Learning - Random Forest Classifier (Handles NaN Labels & Saves Feature Columns)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 📂 Define File Paths\n",
    "merged_eeg_path = r\"C:\\Users\\Kevin Tran\\Documents\\Project Data\\merged_eeg_data.csv\"\n",
    "model_path = r\"C:\\Users\\Kevin Tran\\Documents\\Project Data\\For Machine Learning\\random_forest_model.pkl\"\n",
    "feature_columns_path = r\"C:\\Users\\Kevin Tran\\Documents\\Project Data\\For Machine Learning\\feature_columns.pkl\"\n",
    "\n",
    "# ✅ Chunk Size (Adjust based on available memory)\n",
    "chunk_size = 50000\n",
    "\n",
    "# ✅ Initialize lists to store chunks\n",
    "X_chunks = []\n",
    "y_chunks = []\n",
    "\n",
    "print(\"📂 Loading EEG data in chunks...\")\n",
    "\n",
    "# Read merged EEG data in chunks\n",
    "for i, chunk in enumerate(pd.read_csv(merged_eeg_path, chunksize=chunk_size)):\n",
    "    print(f\"🔄 Processing chunk {i + 1}...\")\n",
    "\n",
    "    # 🔍 Ignore non-feature columns\n",
    "    feature_cols = [col for col in chunk.columns if col not in [\"eeg_id\", \"file\", \"channel\", \"window\", \"label\"]]\n",
    "    X_chunk = chunk[feature_cols]\n",
    "    y_chunk = chunk[\"label\"]\n",
    "\n",
    "    # 🔄 Handle missing values in features (Fill NaN with column median)\n",
    "    X_chunk.fillna(X_chunk.median(), inplace=True)\n",
    "\n",
    "    # 🔄 Handle missing labels (Fill NaN with most common label)\n",
    "    y_chunk.fillna(y_chunk.mode()[0], inplace=True)  # Replace NaN labels with the most common class\n",
    "\n",
    "    # ✅ Append to lists\n",
    "    X_chunks.append(X_chunk)\n",
    "    y_chunks.append(y_chunk)\n",
    "\n",
    "    # ⏸ Stop early to prevent memory issues (optional)\n",
    "    if i == 10:  \n",
    "        print(\"⏸ Stopping early to avoid memory issues (Adjust if needed)\")\n",
    "        break\n",
    "\n",
    "# ✅ Combine processed chunks\n",
    "X = pd.concat(X_chunks, ignore_index=True)\n",
    "y = pd.concat(y_chunks, ignore_index=True)\n",
    "\n",
    "# 🚀 Final Check: Ensure `X` and `y` have the same number of rows\n",
    "print(f\"✅ Final Data Shape: X={X.shape}, y={y.shape}\")\n",
    "\n",
    "# 🔄 Split into training (80%) and testing (20%)\n",
    "print(\"🔄 Splitting data into training and testing sets...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 🚀 Train and Save Model\n",
    "print(\"🚀 Training Random Forest model...\")\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# ✅ Save trained model\n",
    "joblib.dump(rf_model, model_path)\n",
    "print(f\"✅ Model saved to: {model_path}\")\n",
    "\n",
    "# ✅ Save expected feature columns\n",
    "joblib.dump(list(X_train.columns), feature_columns_path)\n",
    "print(f\"✅ Saved expected feature columns: {feature_columns_path}\")\n",
    "\n",
    "# 🔍 Predict on test set\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# ✅ Evaluate Performance\n",
    "print(\"✅ Model training completed. Evaluating performance...\")\n",
    "print(f\"🎯 Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(\"\\n🔍 Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Loading trained Random Forest model...\n",
      "📂 Scanning for EEG `.parquet` files in C:\\Users\\Kevin Tran\\Documents\\Project Data\\Input files...\n",
      "✅ Found 2 EEG files. Starting predictions...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing EEG Files:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📂 Processing: 1628180742.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing EEG Files: 100%|██████████| 2/2 [00:00<00:00, 14.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔮 Predicting seizures for 1628180742.parquet...\n",
      "🚨 SEIZURE DETECTED in 1628180742.parquet! 🚨\n",
      "✅ Prediction saved: C:\\Users\\Kevin Tran\\Documents\\Project Data\\Predicted_EEGs\\predicted_1628180742.parquet | Status: Seizure Detected\n",
      "\n",
      "📂 Processing: 2237447621.parquet\n",
      "🔮 Predicting seizures for 2237447621.parquet...\n",
      "✅ No seizure detected in 2237447621.parquet.\n",
      "✅ Prediction saved: C:\\Users\\Kevin Tran\\Documents\\Project Data\\Predicted_EEGs\\predicted_2237447621.parquet | Status: No Seizure Detected\n",
      "✅ All predictions completed! Files saved in: C:\\Users\\Kevin Tran\\Documents\\Project Data\\Predicted_EEGs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#WORKING\n",
    "\n",
    "#new process for eeg prediction \n",
    "\n",
    "# EEG Prediction Using Trained Random Forest Model (With Feature Extraction)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.signal import butter, filtfilt, iirnotch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 📌 Define Paths\n",
    "input_folder = r\"C:\\Users\\Kevin Tran\\Documents\\Project Data\\Input files\"\n",
    "output_folder = r\"C:\\Users\\Kevin Tran\\Documents\\Project Data\\Predicted_EEGs\"\n",
    "model_path = r\"C:\\Users\\Kevin Tran\\Documents\\Project Data\\For Machine Learning\\random_forest_model.pkl\"\n",
    "feature_columns_path = r\"C:\\Users\\Kevin Tran\\Documents\\Project Data\\For Machine Learning\\feature_columns.pkl\"\n",
    "\n",
    "# ✅ Ensure Output Folder Exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# ✅ Load Trained Model and Expected Feature Names\n",
    "print(\"🧠 Loading trained Random Forest model...\")\n",
    "rf_model = joblib.load(model_path)\n",
    "expected_features = joblib.load(feature_columns_path)  # Load expected feature names\n",
    "\n",
    "# 📌 EEG Signal Processing Functions\n",
    "def apply_notch_filter(signal, fs=400, freq=60.0, quality_factor=30):\n",
    "    \"\"\"Apply a notch filter to remove 60Hz noise.\"\"\"\n",
    "    b, a = iirnotch(w0=freq, Q=quality_factor, fs=fs)\n",
    "    return filtfilt(b, a, signal)\n",
    "\n",
    "def apply_bandpass_filter(signal, fs=400, lowcut=0.5, highcut=40.0, order=5):\n",
    "    \"\"\"Apply a bandpass filter to keep frequencies between 0.5Hz and 40Hz.\"\"\"\n",
    "    nyquist = 0.5 * fs\n",
    "    low, high = lowcut / nyquist, highcut / nyquist\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return filtfilt(b, a, signal)\n",
    "\n",
    "def normalize_signal(signal):\n",
    "    \"\"\"Normalize EEG signal to have zero mean and unit variance.\"\"\"\n",
    "    return (signal - np.mean(signal)) / np.std(signal)\n",
    "\n",
    "# 📌 Feature Extraction Functions\n",
    "def extract_time_features(signal):\n",
    "    return {\n",
    "        \"mean\": np.mean(signal),\n",
    "        \"variance\": np.var(signal),\n",
    "        \"skewness\": skew(signal),\n",
    "        \"kurtosis\": kurtosis(signal),\n",
    "        \"rms\": np.sqrt(np.mean(signal**2)),\n",
    "        \"zero_crossing_rate\": np.sum(np.diff(np.sign(signal)) != 0) / len(signal),\n",
    "        \"mean_abs\": np.mean(np.abs(signal)),   # Added missing feature\n",
    "        \"diff_rms1\": np.sqrt(np.mean(np.diff(signal) ** 2)),  # Added missing feature\n",
    "        \"diff_rms2\": np.sqrt(np.mean(np.diff(signal, n=2) ** 2))  # Added missing feature\n",
    "    }\n",
    "\n",
    "def extract_frequency_features(signal, fs):\n",
    "    \"\"\"Extract frequency-based features using FFT.\"\"\"\n",
    "    L = len(signal)\n",
    "    Y = np.fft.fft(signal)\n",
    "    P2 = np.abs(Y / L)\n",
    "    P1 = P2[:L // 2 + 1]\n",
    "    P1[1:-1] *= 2\n",
    "    freqs = fs * np.arange(L // 2 + 1) / L\n",
    "\n",
    "    # EEG frequency bands\n",
    "    bands = {\n",
    "        \"delta\": (1, 3),\n",
    "        \"theta\": (4, 7),\n",
    "        \"alpha1\": (8, 9),\n",
    "        \"alpha2\": (10, 12),\n",
    "        \"beta1\": (13, 17),\n",
    "        \"beta2\": (18, 30),\n",
    "        \"gamma1\": (31, 40),\n",
    "        \"gamma2\": (41, 50),\n",
    "        \"higher\": (51, 250),\n",
    "    }\n",
    "\n",
    "    band_powers = {name: np.sum(P1[(freqs >= low) & (freqs <= high)]) for name, (low, high) in bands.items()}\n",
    "    band_powers[\"spectral_entropy\"] = -np.sum(P1 * np.log(P1 + 1e-10))\n",
    "    return band_powers\n",
    "\n",
    "# 📌 Function to Extract Features from EEG Data\n",
    "def extract_features_from_eeg(file_path):\n",
    "    data = pd.read_parquet(file_path)\n",
    "\n",
    "    # Process each EEG channel\n",
    "    all_features = []\n",
    "    for channel in data.columns:\n",
    "        signal = data[channel].values\n",
    "        signal = apply_notch_filter(signal)\n",
    "        signal = apply_bandpass_filter(signal)\n",
    "        signal = normalize_signal(signal)\n",
    "\n",
    "        # Extract time and frequency features\n",
    "        time_features = extract_time_features(signal)\n",
    "        frequency_features = extract_frequency_features(signal, fs=400)\n",
    "\n",
    "        # Combine into one dictionary\n",
    "        combined_features = {**time_features, **frequency_features}\n",
    "        all_features.append(combined_features)\n",
    "\n",
    "    # Convert list of feature dicts into a DataFrame\n",
    "    feature_df = pd.DataFrame(all_features).mean(axis=0).to_frame().T  # Aggregate across channels\n",
    "\n",
    "    return feature_df\n",
    "\n",
    "# 📌 Function to Process & Predict a Single EEG File\n",
    "def process_and_predict(file_path):\n",
    "    try:\n",
    "        print(f\"\\n📂 Processing: {os.path.basename(file_path)}\")\n",
    "\n",
    "        # Extract Features\n",
    "        feature_df = extract_features_from_eeg(file_path)\n",
    "\n",
    "        # Ensure Columns Match Expected Features\n",
    "        missing_features = set(expected_features) - set(feature_df.columns)\n",
    "        extra_features = set(feature_df.columns) - set(expected_features)\n",
    "\n",
    "        if missing_features:\n",
    "            print(f\"⚠️ Missing features: {missing_features}\")\n",
    "            for feat in missing_features:\n",
    "                feature_df[feat] = 0  # Fill missing features with 0\n",
    "\n",
    "        if extra_features:\n",
    "            print(f\"⚠️ Extra features found: {extra_features}\")\n",
    "            feature_df = feature_df[expected_features]  # Keep only expected features\n",
    "\n",
    "        # Reorder features to match model training order\n",
    "        feature_df = feature_df[expected_features]\n",
    "\n",
    "        # Predict using trained model\n",
    "        print(f\"🔮 Predicting seizures for {os.path.basename(file_path)}...\")\n",
    "        prediction = rf_model.predict(feature_df)\n",
    "\n",
    "        # Determine seizure presence\n",
    "        seizure_detected = 1 in prediction  # If any row has 1, seizure is detected\n",
    "\n",
    "        # Display Final Result\n",
    "        if seizure_detected:\n",
    "            print(f\"🚨 SEIZURE DETECTED in {os.path.basename(file_path)}! 🚨\")\n",
    "            seizure_status = \"Seizure Detected\"\n",
    "        else:\n",
    "            print(f\"✅ No seizure detected in {os.path.basename(file_path)}.\")\n",
    "            seizure_status = \"No Seizure Detected\"\n",
    "\n",
    "        # Save predictions\n",
    "        output_file = os.path.join(output_folder, f\"predicted_{os.path.basename(file_path)}\")\n",
    "        feature_df[\"predicted_label\"] = prediction\n",
    "        feature_df.to_parquet(output_file)\n",
    "\n",
    "        return f\"✅ Prediction saved: {output_file} | Status: {seizure_status}\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"❌ Failed: {os.path.basename(file_path)} | Error: {e}\"\n",
    "\n",
    "# 📌 Run EEG Processing and Prediction\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"📂 Scanning for EEG `.parquet` files in {input_folder}...\")\n",
    "    eeg_files = [os.path.join(input_folder, f) for f in os.listdir(input_folder) if f.endswith(\".parquet\")]\n",
    "\n",
    "    if len(eeg_files) == 0:\n",
    "        print(\"🚨 No `.parquet` files found in the input folder! Please check your folder.\")\n",
    "    else:\n",
    "        print(f\"✅ Found {len(eeg_files)} EEG files. Starting predictions...\\n\")\n",
    "        for file in tqdm(eeg_files, desc=\"Processing EEG Files\"):\n",
    "            result = process_and_predict(file)\n",
    "            print(result)  # Show final prediction status\n",
    "        print(f\"✅ All predictions completed! Files saved in: {output_folder}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BUILD 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Processing chunk 1\n",
      "✅ Chunk 1 saved.\n",
      "🔄 Processing chunk 2\n",
      "✅ Chunk 2 saved.\n",
      "🔄 Processing chunk 3\n",
      "✅ Chunk 3 saved.\n",
      "🔄 Processing chunk 4\n",
      "✅ Chunk 4 saved.\n",
      "🔄 Processing chunk 5\n",
      "✅ Chunk 5 saved.\n",
      "🔄 Processing chunk 6\n",
      "✅ Chunk 6 saved.\n",
      "🔄 Processing chunk 7\n",
      "✅ Chunk 7 saved.\n",
      "🔄 Processing chunk 8\n",
      "✅ Chunk 8 saved.\n",
      "🔄 Processing chunk 9\n",
      "✅ Chunk 9 saved.\n",
      "🔄 Processing chunk 10\n",
      "✅ Chunk 10 saved.\n",
      "🔄 Processing chunk 11\n",
      "✅ Chunk 11 saved.\n",
      "🔄 Processing chunk 12\n",
      "✅ Chunk 12 saved.\n",
      "🔄 Processing chunk 13\n",
      "✅ Chunk 13 saved.\n",
      "🔄 Processing chunk 14\n",
      "✅ Chunk 14 saved.\n",
      "🔄 Processing chunk 15\n",
      "✅ Chunk 15 saved.\n",
      "🔄 Processing chunk 16\n",
      "✅ Chunk 16 saved.\n",
      "🔄 Processing chunk 17\n",
      "✅ Chunk 17 saved.\n",
      "🔄 Processing chunk 18\n",
      "✅ Chunk 18 saved.\n",
      "🔄 Processing chunk 19\n",
      "✅ Chunk 19 saved.\n",
      "🔄 Processing chunk 20\n",
      "✅ Chunk 20 saved.\n",
      "🔄 Processing chunk 21\n",
      "✅ Chunk 21 saved.\n",
      "🔄 Processing chunk 22\n",
      "✅ Chunk 22 saved.\n",
      "🔄 Processing chunk 23\n",
      "✅ Chunk 23 saved.\n",
      "🔄 Processing chunk 24\n",
      "✅ Chunk 24 saved.\n",
      "🔄 Processing chunk 25\n",
      "✅ Chunk 25 saved.\n",
      "🔄 Processing chunk 26\n",
      "✅ Chunk 26 saved.\n",
      "🔄 Processing chunk 27\n",
      "✅ Chunk 27 saved.\n",
      "🔄 Processing chunk 28\n",
      "✅ Chunk 28 saved.\n",
      "🔄 Processing chunk 29\n",
      "✅ Chunk 29 saved.\n",
      "🔄 Processing chunk 30\n",
      "✅ Chunk 30 saved.\n",
      "🔄 Processing chunk 31\n",
      "✅ Chunk 31 saved.\n",
      "🔄 Processing chunk 32\n",
      "✅ Chunk 32 saved.\n",
      "🔄 Processing chunk 33\n",
      "✅ Chunk 33 saved.\n",
      "🔄 Processing chunk 34\n",
      "✅ Chunk 34 saved.\n",
      "🔄 Processing chunk 35\n",
      "✅ Chunk 35 saved.\n",
      "🔄 Processing chunk 36\n",
      "✅ Chunk 36 saved.\n",
      "🔄 Processing chunk 37\n",
      "✅ Chunk 37 saved.\n",
      "🔄 Processing chunk 38\n",
      "✅ Chunk 38 saved.\n",
      "🔄 Processing chunk 39\n",
      "✅ Chunk 39 saved.\n",
      "🔄 Processing chunk 40\n",
      "✅ Chunk 40 saved.\n",
      "🔄 Processing chunk 41\n",
      "✅ Chunk 41 saved.\n",
      "🔄 Processing chunk 42\n",
      "✅ Chunk 42 saved.\n",
      "🔄 Processing chunk 43\n",
      "✅ Chunk 43 saved.\n",
      "🔄 Processing chunk 44\n",
      "✅ Chunk 44 saved.\n",
      "🔄 Processing chunk 45\n",
      "✅ Chunk 45 saved.\n",
      "🔄 Processing chunk 46\n",
      "✅ Chunk 46 saved.\n",
      "🔄 Processing chunk 47\n",
      "✅ Chunk 47 saved.\n",
      "🔄 Processing chunk 48\n",
      "✅ Chunk 48 saved.\n",
      "🔄 Processing chunk 49\n",
      "✅ Chunk 49 saved.\n",
      "🔄 Processing chunk 50\n",
      "✅ Chunk 50 saved.\n",
      "🔄 Processing chunk 51\n",
      "✅ Chunk 51 saved.\n",
      "🔄 Processing chunk 52\n",
      "✅ Chunk 52 saved.\n",
      "🔄 Processing chunk 53\n",
      "✅ Chunk 53 saved.\n",
      "🔄 Processing chunk 54\n",
      "✅ Chunk 54 saved.\n",
      "🔄 Processing chunk 55\n",
      "✅ Chunk 55 saved.\n",
      "🔄 Processing chunk 56\n",
      "✅ Chunk 56 saved.\n",
      "🔄 Processing chunk 57\n",
      "✅ Chunk 57 saved.\n",
      "🔄 Processing chunk 58\n",
      "✅ Chunk 58 saved.\n",
      "🔄 Processing chunk 59\n",
      "✅ Chunk 59 saved.\n",
      "🔄 Processing chunk 60\n",
      "✅ Chunk 60 saved.\n",
      "🔄 Processing chunk 61\n",
      "✅ Chunk 61 saved.\n",
      "🔄 Processing chunk 62\n",
      "✅ Chunk 62 saved.\n",
      "🔄 Processing chunk 63\n",
      "✅ Chunk 63 saved.\n",
      "🔄 Processing chunk 64\n",
      "✅ Chunk 64 saved.\n",
      "🔄 Processing chunk 65\n",
      "✅ Chunk 65 saved.\n",
      "🔄 Processing chunk 66\n",
      "✅ Chunk 66 saved.\n",
      "🎉 Merging completed. Data saved at: C:\\Users\\Kevin Tran\\Documents\\Project Data\\merged_eeg_data2.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 📂 Define File Paths\n",
    "eeg_features_path = r\"C:\\Users\\Kevin Tran\\Documents\\Project Data\\feature extractions\\eeg_features.csv\"\n",
    "train_path = r\"C:\\Users\\Kevin Tran\\Documents\\GitHub ED1\\hms-harmful-brain-activity-classificationtrain_eegs\\train.csv\"\n",
    "merged_eeg_path = r\"C:\\Users\\Kevin Tran\\Documents\\Project Data\\merged_eeg_data2.csv\"\n",
    "\n",
    "# ✅ Load train.csv\n",
    "train_df = pd.read_csv(train_path)\n",
    "\n",
    "# 🔄 Convert `expert_consensus` into multi-class labels\n",
    "label_mapping = {\n",
    "    \"Seizure\": 1,\n",
    "    \"GPD\": 2,\n",
    "    \"GRDA\": 3,\n",
    "    \"LPD\": 4,\n",
    "    \"LRDA\": 5,\n",
    "    \"Normal\": 0  # Treat all non-harmful cases as \"Normal\"\n",
    "}\n",
    "\n",
    "train_df[\"label\"] = train_df[\"expert_consensus\"].map(label_mapping)\n",
    "\n",
    "# 🎯 Keep only necessary columns\n",
    "train_df = train_df[[\"eeg_id\", \"label\"]]\n",
    "\n",
    "# 🔄 Ensure `eeg_id` is a string for accurate merging\n",
    "train_df[\"eeg_id\"] = train_df[\"eeg_id\"].astype(str)\n",
    "\n",
    "# 🛠 Define chunk size for memory efficiency\n",
    "chunk_size = 100000\n",
    "\n",
    "# 🚀 Initialize CSV file (overwrite if exists)\n",
    "with open(merged_eeg_path, \"w\", newline=\"\") as f:\n",
    "    pass  # Just creating/clearing the file\n",
    "\n",
    "# 🔄 Process EEG features in chunks and save incrementally\n",
    "for i, chunk in enumerate(pd.read_csv(eeg_features_path, chunksize=chunk_size, dtype={\"file\": str})):\n",
    "    print(f\"🔄 Processing chunk {i + 1}\")\n",
    "\n",
    "    # 🏷 Extract `eeg_id` from 'file' column (remove `.parquet` extension)\n",
    "    chunk[\"eeg_id\"] = chunk[\"file\"].str.replace(\".parquet\", \"\", regex=False)\n",
    "\n",
    "    # 🔄 Merge chunk with `train.csv` labels\n",
    "    merged_chunk = chunk.merge(train_df, on=\"eeg_id\", how=\"left\")\n",
    "\n",
    "    # ✅ Append to CSV (without keeping everything in memory)\n",
    "    merged_chunk.to_csv(merged_eeg_path, mode=\"a\", index=False, header=(i == 0))  # Write header only for first chunk\n",
    "\n",
    "    print(f\"✅ Chunk {i + 1} saved.\")\n",
    "\n",
    "print(f\"🎉 Merging completed. Data saved at: {merged_eeg_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Loading EEG data in chunks...\n",
      "🔄 Processing chunk 1...\n",
      "🔄 Processing chunk 2...\n",
      "🔄 Processing chunk 3...\n",
      "🔄 Processing chunk 4...\n",
      "🔄 Processing chunk 5...\n",
      "🔄 Processing chunk 6...\n",
      "🔄 Processing chunk 7...\n",
      "🔄 Processing chunk 8...\n",
      "🔄 Processing chunk 9...\n",
      "🔄 Processing chunk 10...\n",
      "🔄 Processing chunk 11...\n",
      "⏸ Stopping early to avoid memory issues (Adjust if needed)\n",
      "✅ Final Data Shape: X=(550000, 19), y=(550000,)\n",
      "🔄 Splitting data into training and testing sets...\n",
      "🚀 Training Random Forest model...\n",
      "✅ Model saved to: C:\\Users\\Kevin Tran\\Documents\\Project Data\\For Machine Learning\\random_forest_model2.pkl\n",
      "✅ Saved expected feature columns: C:\\Users\\Kevin Tran\\Documents\\Project Data\\For Machine Learning\\feature_columns2.pkl\n",
      "Unique classes in y_test: [1. 2. 3. 4. 5.]\n",
      "\n",
      "🔍 Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Seizure       0.98      0.97      0.97     38400\n",
      "         GPD       0.97      0.99      0.98     41423\n",
      "        GRDA       0.98      0.97      0.97      4740\n",
      "         LPD       0.96      0.89      0.93      9348\n",
      "        LRDA       0.95      0.98      0.96     16089\n",
      "\n",
      "    accuracy                           0.97    110000\n",
      "   macro avg       0.97      0.96      0.96    110000\n",
      "weighted avg       0.97      0.97      0.97    110000\n",
      "\n",
      "\n",
      "🔍 Confusion Matrix:\n",
      " [[37108  1015    11   243    23]\n",
      " [  341 40994    17    44    27]\n",
      " [   67     4  4581    14    74]\n",
      " [  224    34     5  8360   725]\n",
      " [  187    68    71    47 15716]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 📂 Define File Paths\n",
    "merged_eeg_path = r\"C:\\Users\\Kevin Tran\\Documents\\Project Data\\merged_eeg_data2.csv\"\n",
    "model_path = r\"C:\\Users\\Kevin Tran\\Documents\\Project Data\\For Machine Learning\\random_forest_model2.pkl\"\n",
    "feature_columns_path = r\"C:\\Users\\Kevin Tran\\Documents\\Project Data\\For Machine Learning\\feature_columns2.pkl\"\n",
    "\n",
    "# ✅ Chunk Size (Adjust based on available memory)\n",
    "chunk_size = 50000\n",
    "\n",
    "# ✅ Initialize lists to store chunks\n",
    "X_chunks = []\n",
    "y_chunks = []\n",
    "\n",
    "print(\"📂 Loading EEG data in chunks...\")\n",
    "\n",
    "# Read merged EEG data in chunks\n",
    "for i, chunk in enumerate(pd.read_csv(merged_eeg_path, chunksize=chunk_size)):\n",
    "    print(f\"🔄 Processing chunk {i + 1}...\")\n",
    "\n",
    "    # 🔍 Ignore non-feature columns\n",
    "    feature_cols = [col for col in chunk.columns if col not in [\"eeg_id\", \"file\", \"channel\", \"window\", \"label\"]]\n",
    "    X_chunk = chunk[feature_cols]\n",
    "    y_chunk = chunk[\"label\"]\n",
    "\n",
    "    # 🔄 Handle missing values in features (Fill NaN with column median) - Fixing SettingWithCopyWarning\n",
    "    X_chunk = X_chunk.fillna(X_chunk.median())\n",
    "\n",
    "    # 🔄 Handle missing labels (Fill NaN with most common label)\n",
    "    if y_chunk.isna().sum() > 0:\n",
    "        y_chunk = y_chunk.fillna(y_chunk.mode()[0])  # Replace NaN labels with the most common class\n",
    "\n",
    "    # ✅ Append to lists\n",
    "    X_chunks.append(X_chunk)\n",
    "    y_chunks.append(y_chunk)\n",
    "\n",
    "    # ⏸ Stop early to prevent memory issues (adjust if needed)\n",
    "    if i == 10:\n",
    "        print(\"⏸ Stopping early to avoid memory issues (Adjust if needed)\")\n",
    "        break\n",
    "\n",
    "# ✅ Combine processed chunks\n",
    "X = pd.concat(X_chunks, ignore_index=True)\n",
    "y = pd.concat(y_chunks, ignore_index=True)\n",
    "\n",
    "# 🚀 Final Check: Ensure `X` and `y` have the same number of rows\n",
    "print(f\"✅ Final Data Shape: X={X.shape}, y={y.shape}\")\n",
    "\n",
    "# 🔄 Split into training (80%) and testing (20%)\n",
    "print(\"🔄 Splitting data into training and testing sets...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 🚀 Train and Save Model\n",
    "print(\"🚀 Training Random Forest model...\")\n",
    "rf_model = RandomForestClassifier(n_estimators=200, class_weight=\"balanced\", random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# ✅ Save trained model\n",
    "joblib.dump(rf_model, model_path)\n",
    "print(f\"✅ Model saved to: {model_path}\")\n",
    "\n",
    "# ✅ Save expected feature columns\n",
    "joblib.dump(list(X_train.columns), feature_columns_path)\n",
    "print(f\"✅ Saved expected feature columns: {feature_columns_path}\")\n",
    "\n",
    "# 🔍 Predict on test set\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# ✅ Define Condition Mapping\n",
    "condition_mapping = {\n",
    "    0: \"Normal\",\n",
    "    1: \"Seizure\",\n",
    "    2: \"GPD\",\n",
    "    3: \"GRDA\",\n",
    "    4: \"LPD\",\n",
    "    5: \"LRDA\"\n",
    "}\n",
    "\n",
    "# ✅ Check unique labels before classification report\n",
    "unique_labels = np.unique(y_test)\n",
    "print(f\"Unique classes in y_test: {unique_labels}\")\n",
    "\n",
    "# ✅ Adjust target names dynamically to avoid ValueError\n",
    "filtered_target_names = [condition_mapping[label] for label in unique_labels]\n",
    "\n",
    "# ✅ Generate classification report without mismatched labels\n",
    "print(\"\\n🔍 Classification Report:\\n\", classification_report(y_test, y_pred, target_names=filtered_target_names))\n",
    "print(\"\\n🔍 Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Loading trained Random Forest model...\n",
      "📂 Scanning for EEG `.parquet` files in C:\\Users\\Kevin Tran\\Documents\\Project Data\\Input files...\n",
      "✅ Found 2 EEG files. Starting predictions...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing EEG Files:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📂 Processing: 1628180742.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing EEG Files: 100%|██████████| 2/2 [00:00<00:00, 12.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔮 Predicting condition for 1628180742.parquet...\n",
      "🔮 Predicted Condition: Seizure\n",
      "✅ Prediction saved: C:\\Users\\Kevin Tran\\Documents\\Project Data\\Predicted_EEGs\\predicted_1628180742.parquet | Status: Seizure\n",
      "\n",
      "📂 Processing: 2237447621.parquet\n",
      "🔮 Predicting condition for 2237447621.parquet...\n",
      "🔮 Predicted Condition: Normal\n",
      "✅ Prediction saved: C:\\Users\\Kevin Tran\\Documents\\Project Data\\Predicted_EEGs\\predicted_2237447621.parquet | Status: Normal\n",
      "✅ All predictions completed! Files saved in: C:\\Users\\Kevin Tran\\Documents\\Project Data\\Predicted_EEGs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.signal import butter, filtfilt, iirnotch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 📌 Define Paths\n",
    "input_folder = r\"C:\\Users\\Kevin Tran\\Documents\\Project Data\\Input files\"\n",
    "output_folder = r\"C:\\Users\\Kevin Tran\\Documents\\Project Data\\Predicted_EEGs\"\n",
    "model_path = r\"C:\\Users\\Kevin Tran\\Documents\\Project Data\\For Machine Learning\\random_forest_model.pkl\"\n",
    "feature_columns_path = r\"C:\\Users\\Kevin Tran\\Documents\\Project Data\\For Machine Learning\\feature_columns.pkl\"\n",
    "\n",
    "# ✅ Ensure Output Folder Exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# ✅ Load Trained Model and Expected Feature Names\n",
    "print(\"🧠 Loading trained Random Forest model...\")\n",
    "rf_model = joblib.load(model_path)\n",
    "expected_features = joblib.load(feature_columns_path)  # Load expected feature names\n",
    "\n",
    "# ✅ Define Multi-Class Condition Mapping (Same as in Training)\n",
    "condition_mapping = {\n",
    "    0: \"Normal\",\n",
    "    1: \"Seizure\",\n",
    "    2: \"GPD\",\n",
    "    3: \"GRDA\",\n",
    "    4: \"LPD\",\n",
    "    5: \"LRDA\"\n",
    "}\n",
    "\n",
    "# 📌 EEG Signal Processing Functions\n",
    "def apply_notch_filter(signal, fs=400, freq=60.0, quality_factor=30):\n",
    "    \"\"\"Apply a notch filter to remove 60Hz noise.\"\"\"\n",
    "    b, a = iirnotch(w0=freq, Q=quality_factor, fs=fs)\n",
    "    return filtfilt(b, a, signal)\n",
    "\n",
    "def apply_bandpass_filter(signal, fs=400, lowcut=0.5, highcut=40.0, order=5):\n",
    "    \"\"\"Apply a bandpass filter to keep frequencies between 0.5Hz and 40Hz.\"\"\"\n",
    "    nyquist = 0.5 * fs\n",
    "    low, high = lowcut / nyquist, highcut / nyquist\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return filtfilt(b, a, signal)\n",
    "\n",
    "def normalize_signal(signal):\n",
    "    \"\"\"Normalize EEG signal to have zero mean and unit variance.\"\"\"\n",
    "    return (signal - np.mean(signal)) / np.std(signal)\n",
    "\n",
    "# 📌 Feature Extraction Functions\n",
    "def extract_time_features(signal):\n",
    "    return {\n",
    "        \"mean\": np.mean(signal),\n",
    "        \"variance\": np.var(signal),\n",
    "        \"skewness\": skew(signal),\n",
    "        \"kurtosis\": kurtosis(signal),\n",
    "        \"rms\": np.sqrt(np.mean(signal**2)),\n",
    "        \"zero_crossing_rate\": np.sum(np.diff(np.sign(signal)) != 0) / len(signal),\n",
    "        \"mean_abs\": np.mean(np.abs(signal)),\n",
    "        \"diff_rms1\": np.sqrt(np.mean(np.diff(signal) ** 2)),\n",
    "        \"diff_rms2\": np.sqrt(np.mean(np.diff(signal, n=2) ** 2))\n",
    "    }\n",
    "\n",
    "def extract_frequency_features(signal, fs):\n",
    "    \"\"\"Extract frequency-based features using FFT.\"\"\"\n",
    "    L = len(signal)\n",
    "    Y = np.fft.fft(signal)\n",
    "    P2 = np.abs(Y / L)\n",
    "    P1 = P2[:L // 2 + 1]\n",
    "    P1[1:-1] *= 2\n",
    "    freqs = fs * np.arange(L // 2 + 1) / L\n",
    "\n",
    "    # EEG frequency bands\n",
    "    bands = {\n",
    "        \"delta\": (1, 3),\n",
    "        \"theta\": (4, 7),\n",
    "        \"alpha1\": (8, 9),\n",
    "        \"alpha2\": (10, 12),\n",
    "        \"beta1\": (13, 17),\n",
    "        \"beta2\": (18, 30),\n",
    "        \"gamma1\": (31, 40),\n",
    "        \"gamma2\": (41, 50),\n",
    "        \"higher\": (51, 250),\n",
    "    }\n",
    "\n",
    "    band_powers = {name: np.sum(P1[(freqs >= low) & (freqs <= high)]) for name, (low, high) in bands.items()}\n",
    "    band_powers[\"spectral_entropy\"] = -np.sum(P1 * np.log(P1 + 1e-10))\n",
    "    return band_powers\n",
    "\n",
    "# 📌 Function to Extract Features from EEG Data\n",
    "def extract_features_from_eeg(file_path):\n",
    "    data = pd.read_parquet(file_path)\n",
    "\n",
    "    # Process each EEG channel\n",
    "    all_features = []\n",
    "    for channel in data.columns:\n",
    "        signal = data[channel].values\n",
    "        signal = apply_notch_filter(signal)\n",
    "        signal = apply_bandpass_filter(signal)\n",
    "        signal = normalize_signal(signal)\n",
    "\n",
    "        # Extract time and frequency features\n",
    "        time_features = extract_time_features(signal)\n",
    "        frequency_features = extract_frequency_features(signal, fs=400)\n",
    "\n",
    "        # Combine into one dictionary\n",
    "        combined_features = {**time_features, **frequency_features}\n",
    "        all_features.append(combined_features)\n",
    "\n",
    "    # Convert list of feature dicts into a DataFrame\n",
    "    feature_df = pd.DataFrame(all_features).mean(axis=0).to_frame().T  # Aggregate across channels\n",
    "\n",
    "    return feature_df\n",
    "\n",
    "# 📌 Function to Process & Predict a Single EEG File\n",
    "def process_and_predict(file_path):\n",
    "    try:\n",
    "        print(f\"\\n📂 Processing: {os.path.basename(file_path)}\")\n",
    "\n",
    "        # Extract Features\n",
    "        feature_df = extract_features_from_eeg(file_path)\n",
    "\n",
    "        # Ensure Columns Match Expected Features\n",
    "        missing_features = set(expected_features) - set(feature_df.columns)\n",
    "        extra_features = set(feature_df.columns) - set(expected_features)\n",
    "\n",
    "        if missing_features:\n",
    "            print(f\"⚠️ Missing features: {missing_features}\")\n",
    "            for feat in missing_features:\n",
    "                feature_df[feat] = 0  # Fill missing features with 0\n",
    "\n",
    "        if extra_features:\n",
    "            print(f\"⚠️ Extra features found: {extra_features}\")\n",
    "            feature_df = feature_df[expected_features]  # Keep only expected features\n",
    "\n",
    "        # Reorder features to match model training order\n",
    "        feature_df = feature_df[expected_features]\n",
    "\n",
    "        # Predict using trained model\n",
    "        print(f\"🔮 Predicting condition for {os.path.basename(file_path)}...\")\n",
    "        prediction = rf_model.predict(feature_df)\n",
    "\n",
    "        # Convert numeric prediction to condition label\n",
    "        predicted_condition = condition_mapping[int(prediction[0])]\n",
    "\n",
    "        print(f\"🔮 Predicted Condition: {predicted_condition}\")\n",
    "\n",
    "        # Save predictions\n",
    "        output_file = os.path.join(output_folder, f\"predicted_{os.path.basename(file_path)}\")\n",
    "        feature_df[\"predicted_label\"] = prediction\n",
    "        feature_df.to_parquet(output_file)\n",
    "\n",
    "        return f\"✅ Prediction saved: {output_file} | Status: {predicted_condition}\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"❌ Failed: {os.path.basename(file_path)} | Error: {e}\"\n",
    "\n",
    "# 📌 Run EEG Processing and Prediction\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"📂 Scanning for EEG `.parquet` files in {input_folder}...\")\n",
    "    eeg_files = [os.path.join(input_folder, f) for f in os.listdir(input_folder) if f.endswith(\".parquet\")]\n",
    "\n",
    "    if len(eeg_files) == 0:\n",
    "        print(\"🚨 No `.parquet` files found in the input folder! Please check your folder.\")\n",
    "    else:\n",
    "        print(f\"✅ Found {len(eeg_files)} EEG files. Starting predictions...\\n\")\n",
    "        for file in tqdm(eeg_files, desc=\"Processing EEG Files\"):\n",
    "            result = process_and_predict(file)\n",
    "            print(result)  # Show final prediction status\n",
    "        print(f\"✅ All predictions completed! Files saved in: {output_folder}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
